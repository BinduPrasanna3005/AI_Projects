{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "_KgXuI7Fh_Rq",
        "outputId": "8def4a9a-5df2-45f9-e1c4-b41bde91489c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "129/129 [==============================] - 21s 148ms/step - loss: 0.3294 - accuracy: 0.8926 - val_loss: 0.1736 - val_accuracy: 0.9366\n",
            "Epoch 2/100\n",
            "129/129 [==============================] - 16s 122ms/step - loss: 0.0750 - accuracy: 0.9782 - val_loss: 0.1285 - val_accuracy: 0.9552\n",
            "Epoch 3/100\n",
            "129/129 [==============================] - 13s 99ms/step - loss: 0.0135 - accuracy: 0.9973 - val_loss: 0.1344 - val_accuracy: 0.9639\n",
            "Epoch 4/100\n",
            "129/129 [==============================] - 11s 88ms/step - loss: 0.0022 - accuracy: 1.0000 - val_loss: 0.1349 - val_accuracy: 0.9617\n",
            "Epoch 5/100\n",
            "129/129 [==============================] - 12s 91ms/step - loss: 9.2419e-04 - accuracy: 1.0000 - val_loss: 0.1404 - val_accuracy: 0.9628\n",
            "Epoch 6/100\n",
            "129/129 [==============================] - 12s 97ms/step - loss: 5.4802e-04 - accuracy: 1.0000 - val_loss: 0.1434 - val_accuracy: 0.9607\n",
            "Epoch 7/100\n",
            "129/129 [==============================] - 13s 97ms/step - loss: 3.7064e-04 - accuracy: 1.0000 - val_loss: 0.1476 - val_accuracy: 0.9552\n",
            "Epoch 8/100\n",
            "129/129 [==============================] - 13s 97ms/step - loss: 2.6582e-04 - accuracy: 1.0000 - val_loss: 0.1506 - val_accuracy: 0.9552\n",
            "Epoch 9/100\n",
            "129/129 [==============================] - 13s 98ms/step - loss: 1.9976e-04 - accuracy: 1.0000 - val_loss: 0.1531 - val_accuracy: 0.9574\n",
            "Epoch 10/100\n",
            "129/129 [==============================] - 12s 97ms/step - loss: 1.5554e-04 - accuracy: 1.0000 - val_loss: 0.1560 - val_accuracy: 0.9563\n",
            "Epoch 11/100\n",
            "129/129 [==============================] - 12s 93ms/step - loss: 1.2404e-04 - accuracy: 1.0000 - val_loss: 0.1583 - val_accuracy: 0.9552\n",
            "Epoch 12/100\n",
            "129/129 [==============================] - 12s 89ms/step - loss: 1.0103e-04 - accuracy: 1.0000 - val_loss: 0.1604 - val_accuracy: 0.9541\n",
            "Epoch 13/100\n",
            "129/129 [==============================] - 13s 97ms/step - loss: 8.3492e-05 - accuracy: 1.0000 - val_loss: 0.1623 - val_accuracy: 0.9541\n",
            "Epoch 14/100\n",
            "129/129 [==============================] - 13s 97ms/step - loss: 7.0024e-05 - accuracy: 1.0000 - val_loss: 0.1646 - val_accuracy: 0.9541\n",
            "Epoch 15/100\n",
            "129/129 [==============================] - 14s 107ms/step - loss: 5.9302e-05 - accuracy: 1.0000 - val_loss: 0.1664 - val_accuracy: 0.9530\n",
            "Epoch 16/100\n",
            "129/129 [==============================] - 17s 132ms/step - loss: 5.0815e-05 - accuracy: 1.0000 - val_loss: 0.1682 - val_accuracy: 0.9519\n",
            "Epoch 17/100\n",
            "129/129 [==============================] - 17s 131ms/step - loss: 4.3824e-05 - accuracy: 1.0000 - val_loss: 0.1697 - val_accuracy: 0.9519\n",
            "Epoch 18/100\n",
            "129/129 [==============================] - 12s 95ms/step - loss: 3.8072e-05 - accuracy: 1.0000 - val_loss: 0.1715 - val_accuracy: 0.9519\n",
            "Epoch 19/100\n",
            "129/129 [==============================] - 12s 96ms/step - loss: 3.3272e-05 - accuracy: 1.0000 - val_loss: 0.1732 - val_accuracy: 0.9519\n",
            "Epoch 20/100\n",
            "129/129 [==============================] - 12s 94ms/step - loss: 2.9227e-05 - accuracy: 1.0000 - val_loss: 0.1746 - val_accuracy: 0.9508\n",
            "Epoch 21/100\n",
            "129/129 [==============================] - 13s 99ms/step - loss: 2.5813e-05 - accuracy: 1.0000 - val_loss: 0.1762 - val_accuracy: 0.9508\n",
            "Epoch 22/100\n",
            "129/129 [==============================] - 13s 100ms/step - loss: 2.2874e-05 - accuracy: 1.0000 - val_loss: 0.1776 - val_accuracy: 0.9508\n",
            "Epoch 23/100\n",
            "129/129 [==============================] - 12s 96ms/step - loss: 2.0373e-05 - accuracy: 1.0000 - val_loss: 0.1791 - val_accuracy: 0.9508\n",
            "Epoch 24/100\n",
            "129/129 [==============================] - 12s 90ms/step - loss: 1.8170e-05 - accuracy: 1.0000 - val_loss: 0.1805 - val_accuracy: 0.9508\n",
            "Epoch 25/100\n",
            "129/129 [==============================] - 12s 94ms/step - loss: 1.6303e-05 - accuracy: 1.0000 - val_loss: 0.1818 - val_accuracy: 0.9508\n",
            "Epoch 26/100\n",
            "129/129 [==============================] - 13s 97ms/step - loss: 1.4631e-05 - accuracy: 1.0000 - val_loss: 0.1833 - val_accuracy: 0.9486\n",
            "Epoch 27/100\n",
            "129/129 [==============================] - 13s 98ms/step - loss: 1.3175e-05 - accuracy: 1.0000 - val_loss: 0.1845 - val_accuracy: 0.9486\n",
            "Epoch 28/100\n",
            "129/129 [==============================] - 13s 98ms/step - loss: 1.1887e-05 - accuracy: 1.0000 - val_loss: 0.1859 - val_accuracy: 0.9486\n",
            "Epoch 29/100\n",
            "129/129 [==============================] - 13s 98ms/step - loss: 1.0765e-05 - accuracy: 1.0000 - val_loss: 0.1872 - val_accuracy: 0.9486\n",
            "Epoch 30/100\n",
            "129/129 [==============================] - 13s 98ms/step - loss: 9.7489e-06 - accuracy: 1.0000 - val_loss: 0.1884 - val_accuracy: 0.9486\n",
            "Epoch 31/100\n",
            "129/129 [==============================] - 12s 97ms/step - loss: 8.8501e-06 - accuracy: 1.0000 - val_loss: 0.1895 - val_accuracy: 0.9486\n",
            "Epoch 32/100\n",
            "129/129 [==============================] - 11s 88ms/step - loss: 8.0475e-06 - accuracy: 1.0000 - val_loss: 0.1907 - val_accuracy: 0.9486\n",
            "Epoch 33/100\n",
            "129/129 [==============================] - 12s 93ms/step - loss: 7.3296e-06 - accuracy: 1.0000 - val_loss: 0.1921 - val_accuracy: 0.9475\n",
            "Epoch 34/100\n",
            "129/129 [==============================] - 12s 96ms/step - loss: 6.6879e-06 - accuracy: 1.0000 - val_loss: 0.1933 - val_accuracy: 0.9475\n",
            "Epoch 35/100\n",
            "129/129 [==============================] - 13s 100ms/step - loss: 6.1085e-06 - accuracy: 1.0000 - val_loss: 0.1945 - val_accuracy: 0.9475\n",
            "Epoch 36/100\n",
            "129/129 [==============================] - 13s 99ms/step - loss: 5.5832e-06 - accuracy: 1.0000 - val_loss: 0.1956 - val_accuracy: 0.9475\n",
            "Epoch 37/100\n",
            "129/129 [==============================] - 13s 100ms/step - loss: 5.1096e-06 - accuracy: 1.0000 - val_loss: 0.1969 - val_accuracy: 0.9475\n",
            "Epoch 38/100\n",
            "129/129 [==============================] - 13s 99ms/step - loss: 4.6816e-06 - accuracy: 1.0000 - val_loss: 0.1980 - val_accuracy: 0.9475\n",
            "Epoch 39/100\n",
            "129/129 [==============================] - 12s 96ms/step - loss: 4.2966e-06 - accuracy: 1.0000 - val_loss: 0.1991 - val_accuracy: 0.9486\n",
            "Epoch 40/100\n",
            "129/129 [==============================] - 12s 90ms/step - loss: 3.9415e-06 - accuracy: 1.0000 - val_loss: 0.2003 - val_accuracy: 0.9464\n",
            "Epoch 41/100\n",
            "129/129 [==============================] - 12s 95ms/step - loss: 3.6249e-06 - accuracy: 1.0000 - val_loss: 0.2014 - val_accuracy: 0.9464\n",
            "Epoch 42/100\n",
            "129/129 [==============================] - 13s 99ms/step - loss: 3.3364e-06 - accuracy: 1.0000 - val_loss: 0.2025 - val_accuracy: 0.9464\n",
            "Epoch 43/100\n",
            "129/129 [==============================] - 12s 95ms/step - loss: 3.0694e-06 - accuracy: 1.0000 - val_loss: 0.2036 - val_accuracy: 0.9464\n",
            "Epoch 44/100\n",
            "129/129 [==============================] - 13s 99ms/step - loss: 2.8232e-06 - accuracy: 1.0000 - val_loss: 0.2048 - val_accuracy: 0.9464\n",
            "Epoch 45/100\n",
            "129/129 [==============================] - 13s 98ms/step - loss: 2.6031e-06 - accuracy: 1.0000 - val_loss: 0.2059 - val_accuracy: 0.9464\n",
            "Epoch 46/100\n",
            "129/129 [==============================] - 13s 98ms/step - loss: 2.4002e-06 - accuracy: 1.0000 - val_loss: 0.2071 - val_accuracy: 0.9464\n",
            "Epoch 47/100\n",
            "129/129 [==============================] - 12s 96ms/step - loss: 2.2161e-06 - accuracy: 1.0000 - val_loss: 0.2081 - val_accuracy: 0.9464\n",
            "Epoch 48/100\n",
            "129/129 [==============================] - 12s 93ms/step - loss: 2.0451e-06 - accuracy: 1.0000 - val_loss: 0.2092 - val_accuracy: 0.9464\n",
            "Epoch 49/100\n",
            "129/129 [==============================] - 12s 91ms/step - loss: 1.8872e-06 - accuracy: 1.0000 - val_loss: 0.2104 - val_accuracy: 0.9464\n",
            "Epoch 50/100\n",
            "129/129 [==============================] - 13s 97ms/step - loss: 1.7446e-06 - accuracy: 1.0000 - val_loss: 0.2114 - val_accuracy: 0.9464\n",
            "Epoch 51/100\n",
            "129/129 [==============================] - 12s 97ms/step - loss: 1.6137e-06 - accuracy: 1.0000 - val_loss: 0.2125 - val_accuracy: 0.9464\n",
            "Epoch 52/100\n",
            "129/129 [==============================] - 13s 99ms/step - loss: 1.4930e-06 - accuracy: 1.0000 - val_loss: 0.2136 - val_accuracy: 0.9464\n",
            "Epoch 53/100\n",
            "129/129 [==============================] - 13s 99ms/step - loss: 1.3813e-06 - accuracy: 1.0000 - val_loss: 0.2146 - val_accuracy: 0.9464\n",
            "Epoch 54/100\n",
            "129/129 [==============================] - 12s 95ms/step - loss: 1.2790e-06 - accuracy: 1.0000 - val_loss: 0.2158 - val_accuracy: 0.9454\n",
            "Epoch 55/100\n",
            "129/129 [==============================] - 12s 94ms/step - loss: 1.1846e-06 - accuracy: 1.0000 - val_loss: 0.2169 - val_accuracy: 0.9454\n",
            "Epoch 56/100\n",
            "129/129 [==============================] - 11s 87ms/step - loss: 1.0978e-06 - accuracy: 1.0000 - val_loss: 0.2179 - val_accuracy: 0.9464\n",
            "Epoch 57/100\n",
            "129/129 [==============================] - 12s 95ms/step - loss: 1.0174e-06 - accuracy: 1.0000 - val_loss: 0.2190 - val_accuracy: 0.9464\n",
            "Epoch 58/100\n",
            "129/129 [==============================] - 13s 99ms/step - loss: 9.4405e-07 - accuracy: 1.0000 - val_loss: 0.2200 - val_accuracy: 0.9454\n",
            "Epoch 59/100\n",
            "129/129 [==============================] - 13s 99ms/step - loss: 8.7596e-07 - accuracy: 1.0000 - val_loss: 0.2211 - val_accuracy: 0.9454\n",
            "Epoch 60/100\n",
            "129/129 [==============================] - 13s 99ms/step - loss: 8.1306e-07 - accuracy: 1.0000 - val_loss: 0.2222 - val_accuracy: 0.9454\n",
            "Epoch 61/100\n",
            "129/129 [==============================] - 13s 99ms/step - loss: 7.5525e-07 - accuracy: 1.0000 - val_loss: 0.2233 - val_accuracy: 0.9454\n",
            "Epoch 62/100\n",
            "129/129 [==============================] - 13s 100ms/step - loss: 7.0146e-07 - accuracy: 1.0000 - val_loss: 0.2243 - val_accuracy: 0.9454\n",
            "Epoch 63/100\n",
            "129/129 [==============================] - 13s 100ms/step - loss: 6.5187e-07 - accuracy: 1.0000 - val_loss: 0.2252 - val_accuracy: 0.9454\n",
            "Epoch 64/100\n",
            "129/129 [==============================] - 12s 95ms/step - loss: 6.0565e-07 - accuracy: 1.0000 - val_loss: 0.2263 - val_accuracy: 0.9454\n",
            "Epoch 65/100\n",
            "129/129 [==============================] - 12s 90ms/step - loss: 5.6283e-07 - accuracy: 1.0000 - val_loss: 0.2274 - val_accuracy: 0.9454\n",
            "Epoch 66/100\n",
            "129/129 [==============================] - 12s 96ms/step - loss: 5.2369e-07 - accuracy: 1.0000 - val_loss: 0.2285 - val_accuracy: 0.9454\n",
            "Epoch 67/100\n",
            "129/129 [==============================] - 13s 98ms/step - loss: 4.8696e-07 - accuracy: 1.0000 - val_loss: 0.2295 - val_accuracy: 0.9454\n",
            "Epoch 68/100\n",
            "129/129 [==============================] - 13s 98ms/step - loss: 4.5330e-07 - accuracy: 1.0000 - val_loss: 0.2305 - val_accuracy: 0.9454\n",
            "Epoch 69/100\n",
            "129/129 [==============================] - 13s 100ms/step - loss: 4.2186e-07 - accuracy: 1.0000 - val_loss: 0.2315 - val_accuracy: 0.9454\n",
            "Epoch 70/100\n",
            "129/129 [==============================] - 13s 99ms/step - loss: 3.9266e-07 - accuracy: 1.0000 - val_loss: 0.2326 - val_accuracy: 0.9454\n",
            "Epoch 71/100\n",
            "129/129 [==============================] - 13s 99ms/step - loss: 3.6601e-07 - accuracy: 1.0000 - val_loss: 0.2336 - val_accuracy: 0.9454\n",
            "Epoch 72/100\n",
            "129/129 [==============================] - 12s 92ms/step - loss: 3.4084e-07 - accuracy: 1.0000 - val_loss: 0.2346 - val_accuracy: 0.9454\n",
            "Epoch 73/100\n",
            "129/129 [==============================] - 12s 92ms/step - loss: 3.1752e-07 - accuracy: 1.0000 - val_loss: 0.2356 - val_accuracy: 0.9454\n",
            "Epoch 74/100\n",
            "129/129 [==============================] - 13s 97ms/step - loss: 2.9612e-07 - accuracy: 1.0000 - val_loss: 0.2367 - val_accuracy: 0.9454\n",
            "Epoch 75/100\n",
            "129/129 [==============================] - 13s 99ms/step - loss: 2.7614e-07 - accuracy: 1.0000 - val_loss: 0.2377 - val_accuracy: 0.9454\n",
            "Epoch 76/100\n",
            "129/129 [==============================] - 12s 95ms/step - loss: 2.5765e-07 - accuracy: 1.0000 - val_loss: 0.2387 - val_accuracy: 0.9454\n",
            "Epoch 77/100\n",
            "129/129 [==============================] - 12s 95ms/step - loss: 2.4066e-07 - accuracy: 1.0000 - val_loss: 0.2397 - val_accuracy: 0.9454\n",
            "Epoch 78/100\n",
            "129/129 [==============================] - 12s 95ms/step - loss: 2.2453e-07 - accuracy: 1.0000 - val_loss: 0.2407 - val_accuracy: 0.9454\n",
            "Epoch 79/100\n",
            "129/129 [==============================] - 13s 99ms/step - loss: 2.0966e-07 - accuracy: 1.0000 - val_loss: 0.2417 - val_accuracy: 0.9454\n",
            "Epoch 80/100\n",
            "129/129 [==============================] - 12s 95ms/step - loss: 1.9608e-07 - accuracy: 1.0000 - val_loss: 0.2427 - val_accuracy: 0.9454\n",
            "Epoch 81/100\n",
            "129/129 [==============================] - 12s 92ms/step - loss: 1.8315e-07 - accuracy: 1.0000 - val_loss: 0.2437 - val_accuracy: 0.9454\n",
            "Epoch 82/100\n",
            "129/129 [==============================] - 13s 99ms/step - loss: 1.7121e-07 - accuracy: 1.0000 - val_loss: 0.2447 - val_accuracy: 0.9443\n",
            "Epoch 83/100\n",
            "129/129 [==============================] - 13s 100ms/step - loss: 1.6002e-07 - accuracy: 1.0000 - val_loss: 0.2457 - val_accuracy: 0.9443\n",
            "Epoch 84/100\n",
            "129/129 [==============================] - 13s 98ms/step - loss: 1.4969e-07 - accuracy: 1.0000 - val_loss: 0.2466 - val_accuracy: 0.9443\n",
            "Epoch 85/100\n",
            "129/129 [==============================] - 13s 99ms/step - loss: 1.4003e-07 - accuracy: 1.0000 - val_loss: 0.2477 - val_accuracy: 0.9443\n",
            "Epoch 86/100\n",
            "129/129 [==============================] - 13s 100ms/step - loss: 1.3112e-07 - accuracy: 1.0000 - val_loss: 0.2486 - val_accuracy: 0.9432\n",
            "Epoch 87/100\n",
            "129/129 [==============================] - 13s 101ms/step - loss: 1.2281e-07 - accuracy: 1.0000 - val_loss: 0.2495 - val_accuracy: 0.9432\n",
            "Epoch 88/100\n",
            "129/129 [==============================] - 13s 97ms/step - loss: 1.1509e-07 - accuracy: 1.0000 - val_loss: 0.2505 - val_accuracy: 0.9432\n",
            "Epoch 89/100\n",
            "129/129 [==============================] - 12s 93ms/step - loss: 1.0781e-07 - accuracy: 1.0000 - val_loss: 0.2515 - val_accuracy: 0.9432\n",
            "Epoch 90/100\n",
            "129/129 [==============================] - 12s 95ms/step - loss: 1.0106e-07 - accuracy: 1.0000 - val_loss: 0.2524 - val_accuracy: 0.9432\n",
            "Epoch 91/100\n",
            "129/129 [==============================] - 13s 101ms/step - loss: 9.4803e-08 - accuracy: 1.0000 - val_loss: 0.2533 - val_accuracy: 0.9432\n",
            "Epoch 92/100\n",
            "129/129 [==============================] - 13s 100ms/step - loss: 8.8938e-08 - accuracy: 1.0000 - val_loss: 0.2543 - val_accuracy: 0.9432\n",
            "Epoch 93/100\n",
            "129/129 [==============================] - 13s 100ms/step - loss: 8.3421e-08 - accuracy: 1.0000 - val_loss: 0.2552 - val_accuracy: 0.9432\n",
            "Epoch 94/100\n",
            "129/129 [==============================] - 13s 100ms/step - loss: 7.8340e-08 - accuracy: 1.0000 - val_loss: 0.2561 - val_accuracy: 0.9432\n",
            "Epoch 95/100\n",
            "129/129 [==============================] - 13s 101ms/step - loss: 7.3629e-08 - accuracy: 1.0000 - val_loss: 0.2570 - val_accuracy: 0.9410\n",
            "Epoch 96/100\n",
            "129/129 [==============================] - 13s 97ms/step - loss: 6.9159e-08 - accuracy: 1.0000 - val_loss: 0.2580 - val_accuracy: 0.9399\n",
            "Epoch 97/100\n",
            "129/129 [==============================] - 13s 101ms/step - loss: 6.5018e-08 - accuracy: 1.0000 - val_loss: 0.2588 - val_accuracy: 0.9399\n",
            "Epoch 98/100\n",
            "129/129 [==============================] - 12s 94ms/step - loss: 6.1133e-08 - accuracy: 1.0000 - val_loss: 0.2598 - val_accuracy: 0.9399\n",
            "Epoch 99/100\n",
            "129/129 [==============================] - 12s 92ms/step - loss: 5.7544e-08 - accuracy: 1.0000 - val_loss: 0.2607 - val_accuracy: 0.9399\n",
            "Epoch 100/100\n",
            "129/129 [==============================] - 13s 99ms/step - loss: 5.4161e-08 - accuracy: 1.0000 - val_loss: 0.2616 - val_accuracy: 0.9399\n",
            "72/72 [==============================] - 1s 11ms/step - loss: 0.2705 - accuracy: 0.9383\n",
            "Test Accuracy: 0.9383202195167542\n",
            "72/72 [==============================] - 1s 16ms/step\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.93      0.95      0.94      1157\n",
            "           1       0.95      0.92      0.94      1129\n",
            "\n",
            "    accuracy                           0.94      2286\n",
            "   macro avg       0.94      0.94      0.94      2286\n",
            "weighted avg       0.94      0.94      0.94      2286\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-20-275c88397454>:99: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  save_model(model, 'cnn.h5')\n"
          ]
        }
      ],
      "source": [
        "#CNN\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Embedding, Conv1D, Dense, GlobalMaxPooling1D, Concatenate\n",
        "from sklearn.metrics import classification_report\n",
        "import numpy as np\n",
        "import joblib\n",
        "from tensorflow.keras.models import save_model\n",
        "\n",
        "# Load dataset from Kaggle path\n",
        "dataset_path = \"dataset_phishing.csv\"\n",
        "dataset = pd.read_csv(dataset_path)\n",
        "\n",
        "# Add URL length as a feature\n",
        "dataset['length_url'] = dataset['url'].apply(len)\n",
        "# Add hostname length as a feature\n",
        "dataset['length_hostname'] = dataset['url'].apply(lambda x: len(x.split('//')[-1].split('/')[0]))\n",
        "\n",
        "# Extract numerical features including IP address presence, DNS record presence, and Google index presence\n",
        "dataset['ip'] = dataset['ip']\n",
        "dataset['dns_record'] = dataset['dns_record']\n",
        "dataset['google_index'] = dataset['google_index']\n",
        "\n",
        "# Encode categorical labels\n",
        "label_encoder = LabelEncoder()\n",
        "dataset['status'] = label_encoder.fit_transform(dataset['status'])\n",
        "\n",
        "# Save the label encoder\n",
        "joblib.dump(label_encoder, 'label_encoder.pkl')\n",
        "\n",
        "# Text preprocessing\n",
        "texts = dataset['url'].values  # Assuming 'url' is the text column\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(texts)\n",
        "sequences = tokenizer.texts_to_sequences(texts)\n",
        "max_text_length = max([len(seq) for seq in sequences])\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "\n",
        "# Save the tokenizer\n",
        "joblib.dump(tokenizer, 'tokenizer.pkl')\n",
        "joblib.dump(max_text_length, 'max_text_length.pkl')\n",
        "\n",
        "# Padding sequences\n",
        "X_text = pad_sequences(sequences, maxlen=max_text_length, padding='post')\n",
        "\n",
        "# Extract numerical features\n",
        "X_num = dataset[['length_url', 'length_hostname', 'ip', 'dns_record', 'google_index']].values\n",
        "scaler = StandardScaler()\n",
        "X_num = scaler.fit_transform(X_num)\n",
        "\n",
        "# Save the scaler\n",
        "joblib.dump(scaler, 'scaler.pkl')\n",
        "\n",
        "# Split features and labels\n",
        "y = dataset['status']\n",
        "X_train_text, X_test_text, X_train_num, X_test_num, y_train, y_test = train_test_split(\n",
        "    X_text, X_num, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Define CNN model for textual features\n",
        "input_text = Input(shape=(max_text_length,))\n",
        "embedding_layer = Embedding(vocab_size, 100)(input_text)\n",
        "conv_layer = Conv1D(filters=128, kernel_size=5, activation='relu')(embedding_layer)\n",
        "pooling_layer = GlobalMaxPooling1D()(conv_layer)\n",
        "\n",
        "# Define dense layer for numerical features\n",
        "input_num = Input(shape=(X_train_num.shape[1],))\n",
        "dense_layer = Dense(64, activation='relu')(input_num)\n",
        "\n",
        "# Concatenate text and numerical features\n",
        "concatenated = Concatenate()([pooling_layer, dense_layer])\n",
        "output = Dense(1, activation='sigmoid')(concatenated)\n",
        "\n",
        "# Define combined model\n",
        "model = Model(inputs=[input_text, input_num], outputs=output)\n",
        "\n",
        "# Compile model\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train model\n",
        "model.fit([X_train_text, X_train_num], y_train, epochs=100, batch_size=64, validation_split=0.1)\n",
        "\n",
        "# Evaluate model\n",
        "loss, accuracy = model.evaluate([X_test_text, X_test_num], y_test)\n",
        "print(f'Test Accuracy: {accuracy}')\n",
        "\n",
        "# Generate predictions for test set\n",
        "y_pred_probs = model.predict([X_test_text, X_test_num])\n",
        "y_pred = (y_pred_probs > 0.5).astype(int)\n",
        "\n",
        "# Print classification report\n",
        "print(classification_report(y_test, y_pred))\n",
        "\n",
        "# Save the trained model\n",
        "save_model(model, 'cnn.h5')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Checking the CNN model\n"
      ],
      "metadata": {
        "id": "WW9d-Jdt9jv9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import joblib\n",
        "from tensorflow.keras.models import load_model\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "import numpy as np\n",
        "import re\n",
        "import pandas as pd\n",
        "\n",
        "# Load the label encoder, tokenizer, max_text_length, and scaler\n",
        "label_encoder = joblib.load('label_encoder.pkl')\n",
        "tokenizer = joblib.load('tokenizer.pkl')\n",
        "max_text_length = joblib.load('max_text_length.pkl')\n",
        "scaler = joblib.load('scaler.pkl')\n",
        "\n",
        "# Load the trained model\n",
        "model = load_model('cnn.h5')\n",
        "\n",
        "# Load dataset from Kaggle path\n",
        "dataset_path = \"dataset_phishing.csv\"\n",
        "dataset = pd.read_csv(dataset_path)\n",
        "\n",
        "# Function to check URL\n",
        "def check_url(url):\n",
        "    # Check if the URL is in the dataset\n",
        "    if url in dataset['url'].values:\n",
        "        # Get features from the dataset\n",
        "        data_row = dataset[dataset['url'] == url].iloc[0]\n",
        "        ip_present = data_row['ip']\n",
        "        dns_record = data_row['dns_record']\n",
        "        google_index = data_row['google_index']\n",
        "    else:\n",
        "        # Compute features based on the URL\n",
        "        ip_present = 1 if re.search(r'\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}', url) is not None else 0\n",
        "        dns_record = 1 if 'dns' in url else 0\n",
        "        google_index = 1 if 'google' in url else 0\n",
        "\n",
        "    # Tokenize and pad the test URL\n",
        "    test_sequence = tokenizer.texts_to_sequences([url])\n",
        "    test_sequence_padded = pad_sequences(test_sequence, maxlen=max_text_length, padding='post')\n",
        "\n",
        "    # Compute URL length\n",
        "    length_url = len(url)\n",
        "    print(f'URL Length: {length_url}')  # Print URL length\n",
        "\n",
        "    # Compute hostname length\n",
        "    length_hostname = len(url.split('//')[-1].split('/')[0])\n",
        "    print(f'Hostname Length: {length_hostname}')  # Print hostname length\n",
        "\n",
        "    # Print IP address presence\n",
        "    print(f'IP Address Presence: {ip_present}')\n",
        "\n",
        "    # Print DNS record presence\n",
        "    print(f'DNS Record Presence: {dns_record}')\n",
        "\n",
        "    # Print Google index presence\n",
        "    print(f'Google Index Presence: {google_index}')\n",
        "\n",
        "    # Scale the URL length, hostname length, IP address presence, DNS record presence, and Google index presence\n",
        "    features = [[length_url, length_hostname, ip_present, dns_record, google_index]]\n",
        "    scaled_features = scaler.transform(features)\n",
        "\n",
        "    # Make prediction\n",
        "    prediction = model.predict([test_sequence_padded, scaled_features])\n",
        "\n",
        "    # Print prediction\n",
        "    if prediction > 0.5:\n",
        "        return \"The URL is predicted to be phishing.\"\n",
        "    else:\n",
        "        return \"The URL is predicted to be legitimate.\"\n",
        "\n",
        "# Example usage\n",
        "test_url = \"http://www.crestonwood.com/router.php\"\n",
        "result = check_url(test_url)\n",
        "print(result)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "IDO4stddiRqE",
        "outputId": "9dd338fa-b636-4492-f11c-594f6a87bbe8"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "URL Length: 37\n",
            "Hostname Length: 19\n",
            "IP Address Presence: 0\n",
            "DNS Record Presence: 1\n",
            "Google Index Presence: 1\n",
            "1/1 [==============================] - 0s 81ms/step\n",
            "The URL is predicted to be legitimate.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# RNN"
      ],
      "metadata": {
        "id": "Jy2Txw_E9pYb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Embedding, SimpleRNN, Dense, Concatenate\n",
        "from sklearn.metrics import classification_report\n",
        "import numpy as np\n",
        "import joblib\n",
        "\n",
        "# Load dataset from Kaggle path\n",
        "dataset_path = \"dataset_phishing.csv\"\n",
        "dataset = pd.read_csv(dataset_path)\n",
        "\n",
        "# Add URL length as a feature\n",
        "dataset['length_url'] = dataset['url'].apply(len)\n",
        "# Add hostname length as a feature\n",
        "dataset['length_hostname'] = dataset['url'].apply(lambda x: len(x.split('//')[-1].split('/')[0]))\n",
        "\n",
        "# Extract numerical features including IP address presence, DNS record presence, and Google index presence\n",
        "dataset['ip'] = dataset['ip']\n",
        "dataset['dns_record'] = dataset['dns_record']\n",
        "dataset['google_index'] = dataset['google_index']\n",
        "\n",
        "# Encode categorical labels\n",
        "label_encoder = LabelEncoder()\n",
        "dataset['status'] = label_encoder.fit_transform(dataset['status'])\n",
        "\n",
        "# Save the label encoder\n",
        "joblib.dump(label_encoder, 'label_encoder.pkl')\n",
        "\n",
        "# Text preprocessing\n",
        "texts = dataset['url'].values  # Assuming 'url' is the text column\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(texts)\n",
        "sequences = tokenizer.texts_to_sequences(texts)\n",
        "max_text_length = max([len(seq) for seq in sequences])\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "\n",
        "# Save the tokenizer\n",
        "joblib.dump(tokenizer, 'tokenizer.pkl')\n",
        "joblib.dump(max_text_length, 'max_text_length.pkl')\n",
        "\n",
        "# Padding sequences\n",
        "X_text = pad_sequences(sequences, maxlen=max_text_length, padding='post')\n",
        "\n",
        "# Extract numerical features\n",
        "X_num = dataset[['length_url', 'length_hostname', 'ip', 'dns_record', 'google_index']].values\n",
        "scaler = StandardScaler()\n",
        "X_num = scaler.fit_transform(X_num)\n",
        "\n",
        "# Save the scaler\n",
        "joblib.dump(scaler, 'scaler.pkl')\n",
        "\n",
        "# Split features and labels\n",
        "y = dataset['status']\n",
        "X_train_text, X_test_text, X_train_num, X_test_num, y_train, y_test = train_test_split(\n",
        "    X_text, X_num, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Define RNN model for textual features\n",
        "input_text = Input(shape=(max_text_length,))\n",
        "embedding_layer = Embedding(vocab_size, 100)(input_text)\n",
        "rnn_layer = SimpleRNN(128)(embedding_layer)\n",
        "\n",
        "# Define dense layer for numerical features\n",
        "input_num = Input(shape=(X_train_num.shape[1],))\n",
        "dense_layer = Dense(64, activation='relu')(input_num)\n",
        "\n",
        "# Concatenate text and numerical features\n",
        "concatenated = Concatenate()([rnn_layer, dense_layer])\n",
        "output = Dense(1, activation='sigmoid')(concatenated)\n",
        "\n",
        "# Define combined model\n",
        "model = Model(inputs=[input_text, input_num], outputs=output)\n",
        "\n",
        "# Compile model\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train model\n",
        "model.fit([X_train_text, X_train_num], y_train, epochs=100, batch_size=64, validation_split=0.1)\n",
        "\n",
        "# Evaluate model\n",
        "loss, accuracy = model.evaluate([X_test_text, X_test_num], y_test)\n",
        "print(f'Test Accuracy: {accuracy}')\n",
        "\n",
        "# Generate predictions for test set\n",
        "y_pred_probs = model.predict([X_test_text, X_test_num])\n",
        "y_pred = (y_pred_probs > 0.5).astype(int)\n",
        "\n",
        "# Print classification report\n",
        "print(classification_report(y_test, y_pred))\n",
        "\n",
        "# Save the trained model\n",
        "save_model(model, 'rnn.h5')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "RqovneH4i4Cd",
        "outputId": "94c05926-84b4-4686-8d0b-ce8cc45a24fb"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "129/129 [==============================] - 14s 94ms/step - loss: 0.4468 - accuracy: 0.7758 - val_loss: 0.3263 - val_accuracy: 0.8689\n",
            "Epoch 2/100\n",
            "129/129 [==============================] - 11s 88ms/step - loss: 0.2321 - accuracy: 0.9063 - val_loss: 0.3444 - val_accuracy: 0.8514\n",
            "Epoch 3/100\n",
            "129/129 [==============================] - 11s 89ms/step - loss: 0.2598 - accuracy: 0.8955 - val_loss: 0.3247 - val_accuracy: 0.8634\n",
            "Epoch 4/100\n",
            "129/129 [==============================] - 12s 94ms/step - loss: 0.2242 - accuracy: 0.9068 - val_loss: 0.3355 - val_accuracy: 0.8568\n",
            "Epoch 5/100\n",
            "129/129 [==============================] - 12s 95ms/step - loss: 0.1081 - accuracy: 0.9605 - val_loss: 0.3593 - val_accuracy: 0.8710\n",
            "Epoch 6/100\n",
            "129/129 [==============================] - 12s 94ms/step - loss: 0.0458 - accuracy: 0.9861 - val_loss: 0.4499 - val_accuracy: 0.8262\n",
            "Epoch 7/100\n",
            "129/129 [==============================] - 12s 95ms/step - loss: 0.0182 - accuracy: 0.9947 - val_loss: 0.5760 - val_accuracy: 0.8230\n",
            "Epoch 8/100\n",
            "129/129 [==============================] - 12s 92ms/step - loss: 0.0108 - accuracy: 0.9979 - val_loss: 0.5413 - val_accuracy: 0.8317\n",
            "Epoch 9/100\n",
            "129/129 [==============================] - 11s 85ms/step - loss: 0.0223 - accuracy: 0.9940 - val_loss: 0.6622 - val_accuracy: 0.8284\n",
            "Epoch 10/100\n",
            "129/129 [==============================] - 12s 91ms/step - loss: 0.0085 - accuracy: 0.9977 - val_loss: 0.6891 - val_accuracy: 0.8098\n",
            "Epoch 11/100\n",
            "129/129 [==============================] - 12s 94ms/step - loss: 0.0059 - accuracy: 0.9988 - val_loss: 0.6811 - val_accuracy: 0.8175\n",
            "Epoch 12/100\n",
            "129/129 [==============================] - 12s 95ms/step - loss: 0.0052 - accuracy: 0.9987 - val_loss: 0.6794 - val_accuracy: 0.8372\n",
            "Epoch 13/100\n",
            "129/129 [==============================] - 12s 94ms/step - loss: 0.0037 - accuracy: 0.9989 - val_loss: 0.7254 - val_accuracy: 0.8219\n",
            "Epoch 14/100\n",
            "129/129 [==============================] - 12s 95ms/step - loss: 0.0045 - accuracy: 0.9991 - val_loss: 0.7521 - val_accuracy: 0.8142\n",
            "Epoch 15/100\n",
            "129/129 [==============================] - 11s 88ms/step - loss: 0.0014 - accuracy: 0.9999 - val_loss: 0.8091 - val_accuracy: 0.8197\n",
            "Epoch 16/100\n",
            "129/129 [==============================] - 11s 87ms/step - loss: 9.9600e-04 - accuracy: 1.0000 - val_loss: 0.8983 - val_accuracy: 0.8011\n",
            "Epoch 17/100\n",
            "129/129 [==============================] - 12s 93ms/step - loss: 0.0026 - accuracy: 0.9993 - val_loss: 0.9262 - val_accuracy: 0.8044\n",
            "Epoch 18/100\n",
            "129/129 [==============================] - 12s 95ms/step - loss: 0.0014 - accuracy: 0.9996 - val_loss: 1.0236 - val_accuracy: 0.7858\n",
            "Epoch 19/100\n",
            "129/129 [==============================] - 12s 95ms/step - loss: 4.8442e-04 - accuracy: 1.0000 - val_loss: 0.9908 - val_accuracy: 0.8055\n",
            "Epoch 20/100\n",
            "129/129 [==============================] - 12s 96ms/step - loss: 2.5147e-04 - accuracy: 1.0000 - val_loss: 1.0067 - val_accuracy: 0.8098\n",
            "Epoch 21/100\n",
            "129/129 [==============================] - 13s 98ms/step - loss: 5.4144e-04 - accuracy: 0.9999 - val_loss: 1.1054 - val_accuracy: 0.7825\n",
            "Epoch 22/100\n",
            "129/129 [==============================] - 12s 92ms/step - loss: 0.4880 - accuracy: 0.8516 - val_loss: 0.3611 - val_accuracy: 0.8656\n",
            "Epoch 23/100\n",
            "129/129 [==============================] - 11s 87ms/step - loss: 0.3571 - accuracy: 0.8672 - val_loss: 0.3794 - val_accuracy: 0.8590\n",
            "Epoch 24/100\n",
            "129/129 [==============================] - 12s 94ms/step - loss: 0.3512 - accuracy: 0.8678 - val_loss: 0.3579 - val_accuracy: 0.8678\n",
            "Epoch 25/100\n",
            "129/129 [==============================] - 12s 94ms/step - loss: 0.3334 - accuracy: 0.8737 - val_loss: 0.5120 - val_accuracy: 0.8546\n",
            "Epoch 26/100\n",
            "129/129 [==============================] - 12s 96ms/step - loss: 0.2598 - accuracy: 0.8959 - val_loss: 0.3104 - val_accuracy: 0.8754\n",
            "Epoch 27/100\n",
            "129/129 [==============================] - 12s 94ms/step - loss: 0.1835 - accuracy: 0.9249 - val_loss: 0.6149 - val_accuracy: 0.7639\n",
            "Epoch 28/100\n",
            "129/129 [==============================] - 12s 94ms/step - loss: 0.1452 - accuracy: 0.9476 - val_loss: 0.3513 - val_accuracy: 0.8492\n",
            "Epoch 29/100\n",
            "129/129 [==============================] - 12s 92ms/step - loss: 0.1319 - accuracy: 0.9544 - val_loss: 0.4585 - val_accuracy: 0.8022\n",
            "Epoch 30/100\n",
            "129/129 [==============================] - 11s 88ms/step - loss: 0.0630 - accuracy: 0.9793 - val_loss: 0.3056 - val_accuracy: 0.8984\n",
            "Epoch 31/100\n",
            "129/129 [==============================] - 12s 93ms/step - loss: 0.0388 - accuracy: 0.9882 - val_loss: 0.2851 - val_accuracy: 0.8973\n",
            "Epoch 32/100\n",
            "129/129 [==============================] - 12s 96ms/step - loss: 0.0435 - accuracy: 0.9876 - val_loss: 0.4381 - val_accuracy: 0.8568\n",
            "Epoch 33/100\n",
            "129/129 [==============================] - 12s 95ms/step - loss: 0.0489 - accuracy: 0.9837 - val_loss: 0.6554 - val_accuracy: 0.7814\n",
            "Epoch 34/100\n",
            "129/129 [==============================] - 12s 96ms/step - loss: 0.0247 - accuracy: 0.9936 - val_loss: 0.4236 - val_accuracy: 0.8732\n",
            "Epoch 35/100\n",
            "129/129 [==============================] - 12s 96ms/step - loss: 0.0172 - accuracy: 0.9955 - val_loss: 1.0383 - val_accuracy: 0.7311\n",
            "Epoch 36/100\n",
            "129/129 [==============================] - 12s 90ms/step - loss: 0.0125 - accuracy: 0.9967 - val_loss: 0.3963 - val_accuracy: 0.8732\n",
            "Epoch 37/100\n",
            "129/129 [==============================] - 11s 87ms/step - loss: 0.0108 - accuracy: 0.9967 - val_loss: 0.3964 - val_accuracy: 0.8787\n",
            "Epoch 38/100\n",
            "129/129 [==============================] - 12s 96ms/step - loss: 0.0106 - accuracy: 0.9976 - val_loss: 0.7267 - val_accuracy: 0.8339\n",
            "Epoch 39/100\n",
            "129/129 [==============================] - 12s 95ms/step - loss: 0.0219 - accuracy: 0.9939 - val_loss: 0.3126 - val_accuracy: 0.8831\n",
            "Epoch 40/100\n",
            "129/129 [==============================] - 12s 95ms/step - loss: 0.0344 - accuracy: 0.9881 - val_loss: 0.4081 - val_accuracy: 0.8831\n",
            "Epoch 41/100\n",
            "129/129 [==============================] - 12s 95ms/step - loss: 0.0074 - accuracy: 0.9981 - val_loss: 0.3906 - val_accuracy: 0.8940\n",
            "Epoch 42/100\n",
            "129/129 [==============================] - 12s 96ms/step - loss: 0.0254 - accuracy: 0.9923 - val_loss: 1.1702 - val_accuracy: 0.7126\n",
            "Epoch 43/100\n",
            "129/129 [==============================] - 11s 89ms/step - loss: 0.1319 - accuracy: 0.9610 - val_loss: 0.6319 - val_accuracy: 0.7880\n",
            "Epoch 44/100\n",
            "129/129 [==============================] - 11s 87ms/step - loss: 0.0131 - accuracy: 0.9974 - val_loss: 0.4500 - val_accuracy: 0.8667\n",
            "Epoch 45/100\n",
            "129/129 [==============================] - 12s 96ms/step - loss: 0.0085 - accuracy: 0.9981 - val_loss: 0.3775 - val_accuracy: 0.8503\n",
            "Epoch 46/100\n",
            "129/129 [==============================] - 12s 95ms/step - loss: 0.0205 - accuracy: 0.9937 - val_loss: 0.3435 - val_accuracy: 0.8885\n",
            "Epoch 47/100\n",
            "129/129 [==============================] - 12s 95ms/step - loss: 0.0054 - accuracy: 0.9989 - val_loss: 0.5021 - val_accuracy: 0.8612\n",
            "Epoch 48/100\n",
            "129/129 [==============================] - 12s 96ms/step - loss: 0.0048 - accuracy: 0.9990 - val_loss: 0.9911 - val_accuracy: 0.7443\n",
            "Epoch 49/100\n",
            "129/129 [==============================] - 12s 95ms/step - loss: 0.0052 - accuracy: 0.9989 - val_loss: 0.6112 - val_accuracy: 0.8514\n",
            "Epoch 50/100\n",
            "129/129 [==============================] - 12s 91ms/step - loss: 0.0192 - accuracy: 0.9940 - val_loss: 0.6073 - val_accuracy: 0.8546\n",
            "Epoch 51/100\n",
            "129/129 [==============================] - 11s 86ms/step - loss: 0.0243 - accuracy: 0.9921 - val_loss: 0.7415 - val_accuracy: 0.8361\n",
            "Epoch 52/100\n",
            "129/129 [==============================] - 12s 95ms/step - loss: 0.4511 - accuracy: 0.8699 - val_loss: 0.3607 - val_accuracy: 0.8667\n",
            "Epoch 53/100\n",
            "129/129 [==============================] - 12s 96ms/step - loss: 0.3475 - accuracy: 0.8675 - val_loss: 0.3466 - val_accuracy: 0.8678\n",
            "Epoch 54/100\n",
            "129/129 [==============================] - 12s 94ms/step - loss: 0.3449 - accuracy: 0.8684 - val_loss: 0.3472 - val_accuracy: 0.8678\n",
            "Epoch 55/100\n",
            "129/129 [==============================] - 12s 95ms/step - loss: 0.3446 - accuracy: 0.8686 - val_loss: 0.3458 - val_accuracy: 0.8667\n",
            "Epoch 56/100\n",
            "129/129 [==============================] - 12s 96ms/step - loss: 0.3439 - accuracy: 0.8685 - val_loss: 0.3478 - val_accuracy: 0.8656\n",
            "Epoch 57/100\n",
            "129/129 [==============================] - 12s 96ms/step - loss: 0.3432 - accuracy: 0.8689 - val_loss: 0.3479 - val_accuracy: 0.8678\n",
            "Epoch 58/100\n",
            "129/129 [==============================] - 11s 87ms/step - loss: 0.3436 - accuracy: 0.8691 - val_loss: 0.3471 - val_accuracy: 0.8699\n",
            "Epoch 59/100\n",
            "129/129 [==============================] - 12s 90ms/step - loss: 0.3431 - accuracy: 0.8692 - val_loss: 0.3490 - val_accuracy: 0.8699\n",
            "Epoch 60/100\n",
            "129/129 [==============================] - 12s 95ms/step - loss: 0.3426 - accuracy: 0.8680 - val_loss: 0.3522 - val_accuracy: 0.8656\n",
            "Epoch 61/100\n",
            "129/129 [==============================] - 12s 95ms/step - loss: 0.3425 - accuracy: 0.8680 - val_loss: 0.3617 - val_accuracy: 0.8678\n",
            "Epoch 62/100\n",
            "129/129 [==============================] - 12s 95ms/step - loss: 0.3432 - accuracy: 0.8680 - val_loss: 0.3455 - val_accuracy: 0.8678\n",
            "Epoch 63/100\n",
            "129/129 [==============================] - 12s 96ms/step - loss: 0.3417 - accuracy: 0.8688 - val_loss: 0.3457 - val_accuracy: 0.8678\n",
            "Epoch 64/100\n",
            "129/129 [==============================] - 12s 96ms/step - loss: 0.3420 - accuracy: 0.8691 - val_loss: 0.3467 - val_accuracy: 0.8678\n",
            "Epoch 65/100\n",
            "129/129 [==============================] - 11s 87ms/step - loss: 0.3412 - accuracy: 0.8689 - val_loss: 0.3465 - val_accuracy: 0.8678\n",
            "Epoch 66/100\n",
            "129/129 [==============================] - 12s 94ms/step - loss: 0.3417 - accuracy: 0.8690 - val_loss: 0.3479 - val_accuracy: 0.8699\n",
            "Epoch 67/100\n",
            "129/129 [==============================] - 12s 96ms/step - loss: 0.3411 - accuracy: 0.8692 - val_loss: 0.3550 - val_accuracy: 0.8678\n",
            "Epoch 68/100\n",
            "129/129 [==============================] - 12s 96ms/step - loss: 0.3414 - accuracy: 0.8688 - val_loss: 0.3472 - val_accuracy: 0.8678\n",
            "Epoch 69/100\n",
            "129/129 [==============================] - 12s 97ms/step - loss: 0.3410 - accuracy: 0.8692 - val_loss: 0.3529 - val_accuracy: 0.8678\n",
            "Epoch 70/100\n",
            "129/129 [==============================] - 13s 97ms/step - loss: 0.3411 - accuracy: 0.8691 - val_loss: 0.3494 - val_accuracy: 0.8689\n",
            "Epoch 71/100\n",
            "129/129 [==============================] - 12s 96ms/step - loss: 0.3414 - accuracy: 0.8686 - val_loss: 0.3450 - val_accuracy: 0.8678\n",
            "Epoch 72/100\n",
            "129/129 [==============================] - 11s 86ms/step - loss: 0.3400 - accuracy: 0.8688 - val_loss: 0.3496 - val_accuracy: 0.8699\n",
            "Epoch 73/100\n",
            "129/129 [==============================] - 12s 92ms/step - loss: 0.3397 - accuracy: 0.8696 - val_loss: 0.3465 - val_accuracy: 0.8689\n",
            "Epoch 74/100\n",
            "129/129 [==============================] - 12s 96ms/step - loss: 0.3397 - accuracy: 0.8688 - val_loss: 0.3564 - val_accuracy: 0.8656\n",
            "Epoch 75/100\n",
            "129/129 [==============================] - 12s 97ms/step - loss: 0.3394 - accuracy: 0.8685 - val_loss: 0.3466 - val_accuracy: 0.8689\n",
            "Epoch 76/100\n",
            "129/129 [==============================] - 12s 97ms/step - loss: 0.3399 - accuracy: 0.8686 - val_loss: 0.3470 - val_accuracy: 0.8689\n",
            "Epoch 77/100\n",
            "129/129 [==============================] - 12s 96ms/step - loss: 0.3396 - accuracy: 0.8694 - val_loss: 0.3488 - val_accuracy: 0.8699\n",
            "Epoch 78/100\n",
            "129/129 [==============================] - 12s 95ms/step - loss: 0.3396 - accuracy: 0.8685 - val_loss: 0.3450 - val_accuracy: 0.8699\n",
            "Epoch 79/100\n",
            "129/129 [==============================] - 11s 88ms/step - loss: 0.3391 - accuracy: 0.8690 - val_loss: 0.3459 - val_accuracy: 0.8699\n",
            "Epoch 80/100\n",
            "129/129 [==============================] - 12s 91ms/step - loss: 0.3401 - accuracy: 0.8684 - val_loss: 0.3485 - val_accuracy: 0.8699\n",
            "Epoch 81/100\n",
            "129/129 [==============================] - 12s 95ms/step - loss: 0.3389 - accuracy: 0.8689 - val_loss: 0.3464 - val_accuracy: 0.8699\n",
            "Epoch 82/100\n",
            "129/129 [==============================] - 12s 96ms/step - loss: 0.3389 - accuracy: 0.8684 - val_loss: 0.3480 - val_accuracy: 0.8699\n",
            "Epoch 83/100\n",
            "129/129 [==============================] - 12s 95ms/step - loss: 0.3383 - accuracy: 0.8688 - val_loss: 0.3443 - val_accuracy: 0.8678\n",
            "Epoch 84/100\n",
            "129/129 [==============================] - 12s 96ms/step - loss: 0.3394 - accuracy: 0.8691 - val_loss: 0.3449 - val_accuracy: 0.8689\n",
            "Epoch 85/100\n",
            "129/129 [==============================] - 12s 94ms/step - loss: 0.3385 - accuracy: 0.8685 - val_loss: 0.3458 - val_accuracy: 0.8678\n",
            "Epoch 86/100\n",
            "129/129 [==============================] - 11s 87ms/step - loss: 0.3382 - accuracy: 0.8699 - val_loss: 0.3450 - val_accuracy: 0.8667\n",
            "Epoch 87/100\n",
            "129/129 [==============================] - 12s 92ms/step - loss: 0.3376 - accuracy: 0.8688 - val_loss: 0.3449 - val_accuracy: 0.8667\n",
            "Epoch 88/100\n",
            "129/129 [==============================] - 12s 97ms/step - loss: 0.3383 - accuracy: 0.8684 - val_loss: 0.3457 - val_accuracy: 0.8689\n",
            "Epoch 89/100\n",
            "129/129 [==============================] - 13s 97ms/step - loss: 0.3374 - accuracy: 0.8688 - val_loss: 0.3474 - val_accuracy: 0.8689\n",
            "Epoch 90/100\n",
            "129/129 [==============================] - 13s 97ms/step - loss: 0.3372 - accuracy: 0.8688 - val_loss: 0.3496 - val_accuracy: 0.8689\n",
            "Epoch 91/100\n",
            "129/129 [==============================] - 12s 95ms/step - loss: 0.3370 - accuracy: 0.8697 - val_loss: 0.3458 - val_accuracy: 0.8699\n",
            "Epoch 92/100\n",
            "129/129 [==============================] - 12s 97ms/step - loss: 0.3376 - accuracy: 0.8686 - val_loss: 0.3435 - val_accuracy: 0.8667\n",
            "Epoch 93/100\n",
            "129/129 [==============================] - 12s 94ms/step - loss: 0.3365 - accuracy: 0.8691 - val_loss: 0.3434 - val_accuracy: 0.8667\n",
            "Epoch 94/100\n",
            "129/129 [==============================] - 12s 91ms/step - loss: 0.3363 - accuracy: 0.8689 - val_loss: 0.3471 - val_accuracy: 0.8678\n",
            "Epoch 95/100\n",
            "129/129 [==============================] - 12s 92ms/step - loss: 0.3364 - accuracy: 0.8689 - val_loss: 0.3455 - val_accuracy: 0.8678\n",
            "Epoch 96/100\n",
            "129/129 [==============================] - 12s 96ms/step - loss: 0.3367 - accuracy: 0.8690 - val_loss: 0.3499 - val_accuracy: 0.8699\n",
            "Epoch 97/100\n",
            "129/129 [==============================] - 13s 98ms/step - loss: 0.3374 - accuracy: 0.8686 - val_loss: 0.3451 - val_accuracy: 0.8678\n",
            "Epoch 98/100\n",
            "129/129 [==============================] - 12s 97ms/step - loss: 0.3371 - accuracy: 0.8694 - val_loss: 0.3491 - val_accuracy: 0.8699\n",
            "Epoch 99/100\n",
            "129/129 [==============================] - 12s 95ms/step - loss: 0.3365 - accuracy: 0.8696 - val_loss: 0.3447 - val_accuracy: 0.8689\n",
            "Epoch 100/100\n",
            "129/129 [==============================] - 12s 97ms/step - loss: 0.3360 - accuracy: 0.8690 - val_loss: 0.3462 - val_accuracy: 0.8689\n",
            "72/72 [==============================] - 1s 14ms/step - loss: 0.3234 - accuracy: 0.8797\n",
            "Test Accuracy: 0.8797025084495544\n",
            "72/72 [==============================] - 1s 13ms/step\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.92      0.84      0.88      1157\n",
            "           1       0.85      0.92      0.88      1129\n",
            "\n",
            "    accuracy                           0.88      2286\n",
            "   macro avg       0.88      0.88      0.88      2286\n",
            "weighted avg       0.88      0.88      0.88      2286\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-21-f37c0be861f8>:96: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  save_model(model, 'rnn.h5')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Checking the RNN model"
      ],
      "metadata": {
        "id": "i_l6LQK59uoA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import joblib\n",
        "from tensorflow.keras.models import load_model\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "import numpy as np\n",
        "import re\n",
        "import pandas as pd\n",
        "\n",
        "# Load the label encoder, tokenizer, max_text_length, and scaler\n",
        "label_encoder = joblib.load('label_encoder.pkl')\n",
        "tokenizer = joblib.load('tokenizer.pkl')\n",
        "max_text_length = joblib.load('max_text_length.pkl')\n",
        "scaler = joblib.load('scaler.pkl')\n",
        "\n",
        "# Load the trained model\n",
        "model = load_model('rnn.h5')\n",
        "\n",
        "# Load dataset from Kaggle path\n",
        "dataset_path = \"dataset_phishing.csv\"\n",
        "dataset = pd.read_csv(dataset_path)\n",
        "\n",
        "# Function to check URL\n",
        "def check_url(url):\n",
        "    # Check if the URL is in the dataset\n",
        "    if url in dataset['url'].values:\n",
        "        # Get features from the dataset\n",
        "        data_row = dataset[dataset['url'] == url].iloc[0]\n",
        "        ip_present = data_row['ip']\n",
        "        dns_record = data_row['dns_record']\n",
        "        google_index = data_row['google_index']\n",
        "    else:\n",
        "        # Compute features based on the URL\n",
        "        ip_present = 1 if re.search(r'\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}', url) is not None else 0\n",
        "        dns_record = 1 if 'dns' in url else 0\n",
        "        google_index = 1 if 'google' in url else 0\n",
        "\n",
        "    # Tokenize and pad the test URL\n",
        "    test_sequence = tokenizer.texts_to_sequences([url])\n",
        "    test_sequence_padded = pad_sequences(test_sequence, maxlen=max_text_length, padding='post')\n",
        "\n",
        "    # Compute URL length\n",
        "    length_url = len(url)\n",
        "    print(f'URL Length: {length_url}')  # Print URL length\n",
        "\n",
        "    # Compute hostname length\n",
        "    length_hostname = len(url.split('//')[-1].split('/')[0])\n",
        "    print(f'Hostname Length: {length_hostname}')  # Print hostname length\n",
        "\n",
        "    # Print IP address presence\n",
        "    print(f'IP Address Presence: {ip_present}')\n",
        "\n",
        "    # Print DNS record presence\n",
        "    print(f'DNS Record Presence: {dns_record}')\n",
        "\n",
        "    # Print Google index presence\n",
        "    print(f'Google Index Presence: {google_index}')\n",
        "\n",
        "    # Scale the URL length, hostname length, IP address presence, DNS record presence, and Google index presence\n",
        "    features = [[length_url, length_hostname, ip_present, dns_record, google_index]]\n",
        "    scaled_features = scaler.transform(features)\n",
        "\n",
        "    # Make prediction\n",
        "    prediction = model.predict([test_sequence_padded, scaled_features])\n",
        "\n",
        "    # Print prediction\n",
        "    if prediction > 0.5:\n",
        "        return \"The URL is predicted to be phishing.\"\n",
        "    else:\n",
        "        return \"The URL is predicted to be legitimate.\"\n",
        "\n",
        "# Example usage\n",
        "test_url = \"http://www.mutuo.it\"\n",
        "result = check_url(test_url)\n",
        "print(result)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "etEe_zeZjAal",
        "outputId": "d968687f-9a37-4ba7-ffac-68b00f5148ee"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "URL Length: 19\n",
            "Hostname Length: 12\n",
            "IP Address Presence: 0\n",
            "DNS Record Presence: 0\n",
            "Google Index Presence: 0\n",
            "1/1 [==============================] - 0s 337ms/step\n",
            "The URL is predicted to be legitimate.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# RNN with attention mechanism"
      ],
      "metadata": {
        "id": "Hf3PcjdP9zGs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Embedding, LSTM, Dense, Concatenate, Layer\n",
        "from sklearn.metrics import classification_report\n",
        "import numpy as np\n",
        "import joblib\n",
        "from tensorflow.keras.models import save_model\n",
        "import re\n",
        "import tensorflow as tf\n",
        "\n",
        "# Load dataset from Kaggle path\n",
        "dataset_path = \"dataset_phishing.csv\"\n",
        "dataset = pd.read_csv(dataset_path)\n",
        "\n",
        "# Add URL length as a feature\n",
        "dataset['length_url'] = dataset['url'].apply(len)\n",
        "# Add hostname length as a feature\n",
        "dataset['length_hostname'] = dataset['url'].apply(lambda x: len(x.split('//')[-1].split('/')[0]))\n",
        "\n",
        "# Extract numerical features including IP address presence, DNS record presence, and Google index presence\n",
        "dataset['ip'] = dataset['ip']\n",
        "dataset['dns_record'] = dataset['dns_record']\n",
        "dataset['google_index'] = dataset['google_index']\n",
        "\n",
        "# Encode categorical labels\n",
        "label_encoder = LabelEncoder()\n",
        "dataset['status'] = label_encoder.fit_transform(dataset['status'])\n",
        "\n",
        "# Save the label encoder\n",
        "joblib.dump(label_encoder, 'label_encoder.pkl')\n",
        "\n",
        "# Text preprocessing\n",
        "texts = dataset['url'].values  # Assuming 'url' is the text column\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(texts)\n",
        "sequences = tokenizer.texts_to_sequences(texts)\n",
        "max_text_length = max([len(seq) for seq in sequences])\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "\n",
        "# Save the tokenizer\n",
        "joblib.dump(tokenizer, 'tokenizer.pkl')\n",
        "joblib.dump(max_text_length, 'max_text_length.pkl')\n",
        "\n",
        "# Padding sequences\n",
        "X_text = pad_sequences(sequences, maxlen=max_text_length, padding='post')\n",
        "\n",
        "# Extract numerical features\n",
        "X_num = dataset[['length_url', 'length_hostname', 'ip', 'dns_record', 'google_index']].values\n",
        "scaler = StandardScaler()\n",
        "X_num = scaler.fit_transform(X_num)\n",
        "\n",
        "# Save the scaler\n",
        "joblib.dump(scaler, 'scaler.pkl')\n",
        "\n",
        "# Split features and labels\n",
        "y = dataset['status']\n",
        "X_train_text, X_test_text, X_train_num, X_test_num, y_train, y_test = train_test_split(\n",
        "    X_text, X_num, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Define Attention Layer\n",
        "class Attention(Layer):\n",
        "    def __init__(self, **kwargs):\n",
        "        super(Attention, self).__init__(**kwargs)\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        self.W = self.add_weight(name='attention_weight', shape=(input_shape[-1], 1), initializer='random_normal', trainable=True)\n",
        "        self.b = self.add_weight(name='attention_bias', shape=(input_shape[1], 1), initializer='zeros', trainable=True)\n",
        "        super(Attention, self).build(input_shape)\n",
        "\n",
        "    def call(self, x):\n",
        "        e = tf.keras.backend.tanh(tf.keras.backend.dot(x, self.W) + self.b)\n",
        "        a = tf.keras.backend.softmax(e, axis=1)\n",
        "        output = x * a\n",
        "        return tf.keras.backend.sum(output, axis=1)\n",
        "\n",
        "# Define RNN model for textual features with attention\n",
        "input_text = Input(shape=(max_text_length,))\n",
        "embedding_layer = Embedding(vocab_size, 100)(input_text)\n",
        "lstm_layer = LSTM(128, return_sequences=True)(embedding_layer)\n",
        "attention_layer = Attention()(lstm_layer)\n",
        "\n",
        "# Define dense layer for numerical features\n",
        "input_num = Input(shape=(X_train_num.shape[1],))\n",
        "dense_layer = Dense(64, activation='relu')(input_num)\n",
        "\n",
        "# Concatenate text and numerical features\n",
        "concatenated = Concatenate()([attention_layer, dense_layer])\n",
        "output = Dense(1, activation='sigmoid')(concatenated)\n",
        "\n",
        "# Define combined model\n",
        "model = Model(inputs=[input_text, input_num], outputs=output)\n",
        "\n",
        "# Compile model\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train model\n",
        "model.fit([X_train_text, X_train_num], y_train, epochs=100, batch_size=64, validation_split=0.1)\n",
        "\n",
        "# Evaluate model\n",
        "loss, accuracy = model.evaluate([X_test_text, X_test_num], y_test)\n",
        "print(f'Test Accuracy: {accuracy}')\n",
        "\n",
        "# Generate predictions for test set\n",
        "y_pred_probs = model.predict([X_test_text, X_test_num])\n",
        "y_pred = (y_pred_probs > 0.5).astype(int)\n",
        "\n",
        "# Print classification report\n",
        "print(classification_report(y_test, y_pred))\n",
        "\n",
        "# Save the trained model\n",
        "save_model(model, 'rnn_with_attention.h5')\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "JbyO5iaxkkJP",
        "outputId": "56e5e641-81c2-4288-8baf-7a945c469fef"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "129/129 [==============================] - 43s 304ms/step - loss: 0.4363 - accuracy: 0.8205 - val_loss: 0.2927 - val_accuracy: 0.8623\n",
            "Epoch 2/100\n",
            "129/129 [==============================] - 39s 299ms/step - loss: 0.2035 - accuracy: 0.9205 - val_loss: 0.2542 - val_accuracy: 0.9082\n",
            "Epoch 3/100\n",
            "129/129 [==============================] - 36s 283ms/step - loss: 0.1083 - accuracy: 0.9588 - val_loss: 0.2828 - val_accuracy: 0.8776\n",
            "Epoch 4/100\n",
            "129/129 [==============================] - 38s 296ms/step - loss: 0.0559 - accuracy: 0.9827 - val_loss: 0.2928 - val_accuracy: 0.8918\n",
            "Epoch 5/100\n",
            "129/129 [==============================] - 38s 297ms/step - loss: 0.0385 - accuracy: 0.9891 - val_loss: 0.3581 - val_accuracy: 0.8590\n",
            "Epoch 6/100\n",
            "129/129 [==============================] - 37s 286ms/step - loss: 0.0185 - accuracy: 0.9960 - val_loss: 0.4632 - val_accuracy: 0.8667\n",
            "Epoch 7/100\n",
            "129/129 [==============================] - 39s 300ms/step - loss: 0.0110 - accuracy: 0.9979 - val_loss: 0.4693 - val_accuracy: 0.8765\n",
            "Epoch 8/100\n",
            "129/129 [==============================] - 39s 301ms/step - loss: 0.0105 - accuracy: 0.9978 - val_loss: 0.4454 - val_accuracy: 0.8852\n",
            "Epoch 9/100\n",
            "129/129 [==============================] - 39s 306ms/step - loss: 0.0072 - accuracy: 0.9985 - val_loss: 0.4980 - val_accuracy: 0.8907\n",
            "Epoch 10/100\n",
            "129/129 [==============================] - 37s 288ms/step - loss: 0.0209 - accuracy: 0.9950 - val_loss: 0.5336 - val_accuracy: 0.8678\n",
            "Epoch 11/100\n",
            "129/129 [==============================] - 38s 298ms/step - loss: 0.0072 - accuracy: 0.9984 - val_loss: 0.5814 - val_accuracy: 0.8634\n",
            "Epoch 12/100\n",
            "129/129 [==============================] - 39s 306ms/step - loss: 0.0065 - accuracy: 0.9988 - val_loss: 0.4991 - val_accuracy: 0.8842\n",
            "Epoch 13/100\n",
            "129/129 [==============================] - 38s 295ms/step - loss: 0.0418 - accuracy: 0.9870 - val_loss: 0.2910 - val_accuracy: 0.9082\n",
            "Epoch 14/100\n",
            "129/129 [==============================] - 37s 286ms/step - loss: 0.0051 - accuracy: 0.9989 - val_loss: 0.4469 - val_accuracy: 0.8929\n",
            "Epoch 15/100\n",
            "129/129 [==============================] - 39s 300ms/step - loss: 0.0147 - accuracy: 0.9960 - val_loss: 0.4948 - val_accuracy: 0.8896\n",
            "Epoch 16/100\n",
            "129/129 [==============================] - 39s 305ms/step - loss: 0.0025 - accuracy: 0.9996 - val_loss: 0.5487 - val_accuracy: 0.8907\n",
            "Epoch 17/100\n",
            "129/129 [==============================] - 37s 287ms/step - loss: 0.0022 - accuracy: 0.9996 - val_loss: 0.5672 - val_accuracy: 0.8918\n",
            "Epoch 18/100\n",
            "129/129 [==============================] - 39s 299ms/step - loss: 0.0060 - accuracy: 0.9988 - val_loss: 0.5126 - val_accuracy: 0.8896\n",
            "Epoch 19/100\n",
            "129/129 [==============================] - 39s 302ms/step - loss: 0.0047 - accuracy: 0.9989 - val_loss: 0.5590 - val_accuracy: 0.8765\n",
            "Epoch 20/100\n",
            "129/129 [==============================] - 38s 294ms/step - loss: 0.0037 - accuracy: 0.9993 - val_loss: 0.4925 - val_accuracy: 0.8896\n",
            "Epoch 21/100\n",
            "129/129 [==============================] - 37s 286ms/step - loss: 0.0022 - accuracy: 0.9996 - val_loss: 0.6327 - val_accuracy: 0.8765\n",
            "Epoch 22/100\n",
            "129/129 [==============================] - 38s 296ms/step - loss: 0.0040 - accuracy: 0.9993 - val_loss: 0.6724 - val_accuracy: 0.8678\n",
            "Epoch 23/100\n",
            "129/129 [==============================] - 38s 295ms/step - loss: 0.0021 - accuracy: 0.9996 - val_loss: 0.6327 - val_accuracy: 0.8765\n",
            "Epoch 24/100\n",
            "129/129 [==============================] - 37s 286ms/step - loss: 0.0020 - accuracy: 0.9996 - val_loss: 0.6290 - val_accuracy: 0.8765\n",
            "Epoch 25/100\n",
            "129/129 [==============================] - 39s 300ms/step - loss: 0.0020 - accuracy: 0.9996 - val_loss: 0.6446 - val_accuracy: 0.8809\n",
            "Epoch 26/100\n",
            "129/129 [==============================] - 39s 304ms/step - loss: 0.0020 - accuracy: 0.9996 - val_loss: 0.5976 - val_accuracy: 0.8831\n",
            "Epoch 27/100\n",
            "129/129 [==============================] - 38s 293ms/step - loss: 0.0019 - accuracy: 0.9996 - val_loss: 0.5856 - val_accuracy: 0.8874\n",
            "Epoch 28/100\n",
            "129/129 [==============================] - 37s 286ms/step - loss: 0.0014 - accuracy: 0.9998 - val_loss: 0.5503 - val_accuracy: 0.9005\n",
            "Epoch 29/100\n",
            "129/129 [==============================] - 38s 296ms/step - loss: 0.0014 - accuracy: 0.9998 - val_loss: 0.5792 - val_accuracy: 0.9016\n",
            "Epoch 30/100\n",
            "129/129 [==============================] - 40s 307ms/step - loss: 0.0014 - accuracy: 0.9998 - val_loss: 0.5365 - val_accuracy: 0.9038\n",
            "Epoch 31/100\n",
            "129/129 [==============================] - 38s 293ms/step - loss: 0.0020 - accuracy: 0.9995 - val_loss: 0.4816 - val_accuracy: 0.9060\n",
            "Epoch 32/100\n",
            "129/129 [==============================] - 38s 290ms/step - loss: 0.0174 - accuracy: 0.9966 - val_loss: 0.5204 - val_accuracy: 0.8546\n",
            "Epoch 33/100\n",
            "129/129 [==============================] - 39s 301ms/step - loss: 0.0431 - accuracy: 0.9885 - val_loss: 0.3619 - val_accuracy: 0.9060\n",
            "Epoch 34/100\n",
            "129/129 [==============================] - 38s 296ms/step - loss: 0.0066 - accuracy: 0.9989 - val_loss: 0.4495 - val_accuracy: 0.9104\n",
            "Epoch 35/100\n",
            "129/129 [==============================] - 37s 284ms/step - loss: 0.0038 - accuracy: 0.9994 - val_loss: 0.4858 - val_accuracy: 0.9082\n",
            "Epoch 36/100\n",
            "129/129 [==============================] - 38s 297ms/step - loss: 0.0033 - accuracy: 0.9995 - val_loss: 0.5630 - val_accuracy: 0.9005\n",
            "Epoch 37/100\n",
            "129/129 [==============================] - 38s 298ms/step - loss: 0.0028 - accuracy: 0.9995 - val_loss: 0.5160 - val_accuracy: 0.9082\n",
            "Epoch 38/100\n",
            "129/129 [==============================] - 37s 289ms/step - loss: 0.0028 - accuracy: 0.9996 - val_loss: 0.5287 - val_accuracy: 0.9093\n",
            "Epoch 39/100\n",
            "129/129 [==============================] - 39s 302ms/step - loss: 0.0015 - accuracy: 0.9998 - val_loss: 0.5507 - val_accuracy: 0.9093\n",
            "Epoch 40/100\n",
            "129/129 [==============================] - 39s 301ms/step - loss: 9.1753e-04 - accuracy: 0.9999 - val_loss: 0.5790 - val_accuracy: 0.9071\n",
            "Epoch 41/100\n",
            "129/129 [==============================] - 39s 301ms/step - loss: 8.9021e-04 - accuracy: 0.9999 - val_loss: 0.5911 - val_accuracy: 0.9071\n",
            "Epoch 42/100\n",
            "129/129 [==============================] - 37s 287ms/step - loss: 8.6419e-04 - accuracy: 0.9999 - val_loss: 0.5981 - val_accuracy: 0.9060\n",
            "Epoch 43/100\n",
            "129/129 [==============================] - 38s 298ms/step - loss: 8.3873e-04 - accuracy: 0.9999 - val_loss: 0.5985 - val_accuracy: 0.9049\n",
            "Epoch 44/100\n",
            "129/129 [==============================] - 38s 296ms/step - loss: 0.0088 - accuracy: 0.9981 - val_loss: 0.4937 - val_accuracy: 0.8918\n",
            "Epoch 45/100\n",
            "129/129 [==============================] - 37s 287ms/step - loss: 0.0028 - accuracy: 0.9995 - val_loss: 0.6343 - val_accuracy: 0.8863\n",
            "Epoch 46/100\n",
            "129/129 [==============================] - 39s 298ms/step - loss: 0.0011 - accuracy: 0.9999 - val_loss: 0.7356 - val_accuracy: 0.8852\n",
            "Epoch 47/100\n",
            "129/129 [==============================] - 39s 300ms/step - loss: 9.1776e-04 - accuracy: 0.9999 - val_loss: 0.7709 - val_accuracy: 0.8863\n",
            "Epoch 48/100\n",
            "129/129 [==============================] - 38s 292ms/step - loss: 9.0059e-04 - accuracy: 0.9999 - val_loss: 0.7670 - val_accuracy: 0.8874\n",
            "Epoch 49/100\n",
            "129/129 [==============================] - 37s 288ms/step - loss: 8.8456e-04 - accuracy: 0.9999 - val_loss: 0.7410 - val_accuracy: 0.8874\n",
            "Epoch 50/100\n",
            "129/129 [==============================] - 39s 300ms/step - loss: 8.6029e-04 - accuracy: 0.9999 - val_loss: 0.7579 - val_accuracy: 0.8874\n",
            "Epoch 51/100\n",
            "129/129 [==============================] - 39s 302ms/step - loss: 8.6118e-04 - accuracy: 0.9999 - val_loss: 0.7496 - val_accuracy: 0.8918\n",
            "Epoch 52/100\n",
            "129/129 [==============================] - 37s 291ms/step - loss: 8.6274e-04 - accuracy: 0.9999 - val_loss: 0.7474 - val_accuracy: 0.8918\n",
            "Epoch 53/100\n",
            "129/129 [==============================] - 38s 298ms/step - loss: 8.6332e-04 - accuracy: 0.9999 - val_loss: 0.7040 - val_accuracy: 0.8962\n",
            "Epoch 54/100\n",
            "129/129 [==============================] - 38s 298ms/step - loss: 8.5502e-04 - accuracy: 0.9999 - val_loss: 0.6982 - val_accuracy: 0.8951\n",
            "Epoch 55/100\n",
            "129/129 [==============================] - 37s 288ms/step - loss: 8.4249e-04 - accuracy: 0.9999 - val_loss: 0.7022 - val_accuracy: 0.8951\n",
            "Epoch 56/100\n",
            "129/129 [==============================] - 38s 297ms/step - loss: 8.3530e-04 - accuracy: 0.9999 - val_loss: 0.7226 - val_accuracy: 0.8929\n",
            "Epoch 57/100\n",
            "129/129 [==============================] - 39s 300ms/step - loss: 8.3762e-04 - accuracy: 0.9999 - val_loss: 0.6460 - val_accuracy: 0.9005\n",
            "Epoch 58/100\n",
            "129/129 [==============================] - 39s 306ms/step - loss: 8.3062e-04 - accuracy: 0.9999 - val_loss: 0.6347 - val_accuracy: 0.9005\n",
            "Epoch 59/100\n",
            "129/129 [==============================] - 37s 288ms/step - loss: 8.3124e-04 - accuracy: 0.9999 - val_loss: 0.5832 - val_accuracy: 0.9005\n",
            "Epoch 60/100\n",
            "129/129 [==============================] - 39s 301ms/step - loss: 9.8997e-04 - accuracy: 0.9999 - val_loss: 0.8461 - val_accuracy: 0.8874\n",
            "Epoch 61/100\n",
            "129/129 [==============================] - 39s 303ms/step - loss: 8.6210e-04 - accuracy: 0.9999 - val_loss: 0.8165 - val_accuracy: 0.8896\n",
            "Epoch 62/100\n",
            "129/129 [==============================] - 39s 301ms/step - loss: 8.4650e-04 - accuracy: 0.9999 - val_loss: 0.7681 - val_accuracy: 0.8907\n",
            "Epoch 63/100\n",
            "129/129 [==============================] - 37s 285ms/step - loss: 8.2377e-04 - accuracy: 0.9999 - val_loss: 0.7689 - val_accuracy: 0.8907\n",
            "Epoch 64/100\n",
            "129/129 [==============================] - 39s 301ms/step - loss: 8.2292e-04 - accuracy: 0.9999 - val_loss: 0.7325 - val_accuracy: 0.8929\n",
            "Epoch 65/100\n",
            "129/129 [==============================] - 39s 300ms/step - loss: 8.0691e-04 - accuracy: 0.9999 - val_loss: 0.7162 - val_accuracy: 0.8940\n",
            "Epoch 66/100\n",
            "129/129 [==============================] - 37s 286ms/step - loss: 0.0052 - accuracy: 0.9987 - val_loss: 0.5214 - val_accuracy: 0.8568\n",
            "Epoch 67/100\n",
            "129/129 [==============================] - 38s 294ms/step - loss: 0.0420 - accuracy: 0.9847 - val_loss: 0.3318 - val_accuracy: 0.8951\n",
            "Epoch 68/100\n",
            "129/129 [==============================] - 38s 296ms/step - loss: 0.0098 - accuracy: 0.9973 - val_loss: 0.4539 - val_accuracy: 0.9115\n",
            "Epoch 69/100\n",
            "129/129 [==============================] - 39s 304ms/step - loss: 9.0148e-04 - accuracy: 0.9999 - val_loss: 0.5653 - val_accuracy: 0.9060\n",
            "Epoch 70/100\n",
            "129/129 [==============================] - 37s 286ms/step - loss: 0.0012 - accuracy: 0.9998 - val_loss: 0.5292 - val_accuracy: 0.9104\n",
            "Epoch 71/100\n",
            "129/129 [==============================] - 39s 301ms/step - loss: 2.0817e-04 - accuracy: 1.0000 - val_loss: 0.4913 - val_accuracy: 0.9148\n",
            "Epoch 72/100\n",
            "129/129 [==============================] - 38s 298ms/step - loss: 1.4638e-04 - accuracy: 1.0000 - val_loss: 0.4857 - val_accuracy: 0.9202\n",
            "Epoch 73/100\n",
            "129/129 [==============================] - 37s 288ms/step - loss: 1.1042e-04 - accuracy: 1.0000 - val_loss: 0.4850 - val_accuracy: 0.9213\n",
            "Epoch 74/100\n",
            "129/129 [==============================] - 39s 299ms/step - loss: 7.5954e-05 - accuracy: 1.0000 - val_loss: 0.5161 - val_accuracy: 0.9180\n",
            "Epoch 75/100\n",
            "129/129 [==============================] - 39s 301ms/step - loss: 5.9999e-05 - accuracy: 1.0000 - val_loss: 0.5034 - val_accuracy: 0.9224\n",
            "Epoch 76/100\n",
            "129/129 [==============================] - 38s 299ms/step - loss: 3.4854e-05 - accuracy: 1.0000 - val_loss: 0.5127 - val_accuracy: 0.9235\n",
            "Epoch 77/100\n",
            "129/129 [==============================] - 38s 291ms/step - loss: 2.8907e-05 - accuracy: 1.0000 - val_loss: 0.5284 - val_accuracy: 0.9224\n",
            "Epoch 78/100\n",
            "129/129 [==============================] - 39s 303ms/step - loss: 2.4413e-05 - accuracy: 1.0000 - val_loss: 0.5372 - val_accuracy: 0.9224\n",
            "Epoch 79/100\n",
            "129/129 [==============================] - 39s 300ms/step - loss: 2.0922e-05 - accuracy: 1.0000 - val_loss: 0.5516 - val_accuracy: 0.9213\n",
            "Epoch 80/100\n",
            "129/129 [==============================] - 38s 299ms/step - loss: 1.8138e-05 - accuracy: 1.0000 - val_loss: 0.5598 - val_accuracy: 0.9213\n",
            "Epoch 81/100\n",
            "129/129 [==============================] - 37s 289ms/step - loss: 1.5865e-05 - accuracy: 1.0000 - val_loss: 0.5730 - val_accuracy: 0.9202\n",
            "Epoch 82/100\n",
            "129/129 [==============================] - 39s 302ms/step - loss: 1.3993e-05 - accuracy: 1.0000 - val_loss: 0.5803 - val_accuracy: 0.9202\n",
            "Epoch 83/100\n",
            "129/129 [==============================] - 39s 299ms/step - loss: 1.2417e-05 - accuracy: 1.0000 - val_loss: 0.5870 - val_accuracy: 0.9191\n",
            "Epoch 84/100\n",
            "129/129 [==============================] - 37s 289ms/step - loss: 1.1077e-05 - accuracy: 1.0000 - val_loss: 0.5933 - val_accuracy: 0.9191\n",
            "Epoch 85/100\n",
            "129/129 [==============================] - 39s 300ms/step - loss: 9.9291e-06 - accuracy: 1.0000 - val_loss: 0.5993 - val_accuracy: 0.9191\n",
            "Epoch 86/100\n",
            "129/129 [==============================] - 39s 302ms/step - loss: 8.9371e-06 - accuracy: 1.0000 - val_loss: 0.6053 - val_accuracy: 0.9191\n",
            "Epoch 87/100\n",
            "129/129 [==============================] - 39s 301ms/step - loss: 8.0622e-06 - accuracy: 1.0000 - val_loss: 0.6112 - val_accuracy: 0.9180\n",
            "Epoch 88/100\n",
            "129/129 [==============================] - 37s 286ms/step - loss: 7.2854e-06 - accuracy: 1.0000 - val_loss: 0.6173 - val_accuracy: 0.9180\n",
            "Epoch 89/100\n",
            "129/129 [==============================] - 39s 300ms/step - loss: 6.5849e-06 - accuracy: 1.0000 - val_loss: 0.6305 - val_accuracy: 0.9180\n",
            "Epoch 90/100\n",
            "129/129 [==============================] - 38s 299ms/step - loss: 5.9556e-06 - accuracy: 1.0000 - val_loss: 0.6428 - val_accuracy: 0.9180\n",
            "Epoch 91/100\n",
            "129/129 [==============================] - 37s 287ms/step - loss: 5.4170e-06 - accuracy: 1.0000 - val_loss: 0.6486 - val_accuracy: 0.9180\n",
            "Epoch 92/100\n",
            "129/129 [==============================] - 39s 302ms/step - loss: 4.9552e-06 - accuracy: 1.0000 - val_loss: 0.6539 - val_accuracy: 0.9180\n",
            "Epoch 93/100\n",
            "129/129 [==============================] - 39s 300ms/step - loss: 4.5475e-06 - accuracy: 1.0000 - val_loss: 0.6589 - val_accuracy: 0.9180\n",
            "Epoch 94/100\n",
            "129/129 [==============================] - 38s 299ms/step - loss: 4.1809e-06 - accuracy: 1.0000 - val_loss: 0.6639 - val_accuracy: 0.9180\n",
            "Epoch 95/100\n",
            "129/129 [==============================] - 38s 293ms/step - loss: 3.8506e-06 - accuracy: 1.0000 - val_loss: 0.6689 - val_accuracy: 0.9169\n",
            "Epoch 96/100\n",
            "129/129 [==============================] - 39s 300ms/step - loss: 3.5522e-06 - accuracy: 1.0000 - val_loss: 0.6823 - val_accuracy: 0.9158\n",
            "Epoch 97/100\n",
            "129/129 [==============================] - 38s 298ms/step - loss: 3.2804e-06 - accuracy: 1.0000 - val_loss: 0.6891 - val_accuracy: 0.9158\n",
            "Epoch 98/100\n",
            "129/129 [==============================] - 38s 292ms/step - loss: 3.0328e-06 - accuracy: 1.0000 - val_loss: 0.6946 - val_accuracy: 0.9158\n",
            "Epoch 99/100\n",
            "129/129 [==============================] - 39s 299ms/step - loss: 2.8072e-06 - accuracy: 1.0000 - val_loss: 0.6999 - val_accuracy: 0.9148\n",
            "Epoch 100/100\n",
            "129/129 [==============================] - 39s 301ms/step - loss: 2.6007e-06 - accuracy: 1.0000 - val_loss: 0.7119 - val_accuracy: 0.9148\n",
            "72/72 [==============================] - 4s 58ms/step - loss: 0.5165 - accuracy: 0.9344\n",
            "Test Accuracy: 0.93438321352005\n",
            "72/72 [==============================] - 6s 77ms/step\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.94      0.94      0.94      1157\n",
            "           1       0.93      0.93      0.93      1129\n",
            "\n",
            "    accuracy                           0.93      2286\n",
            "   macro avg       0.93      0.93      0.93      2286\n",
            "weighted avg       0.93      0.93      0.93      2286\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-22-663a91eb648c>:116: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  save_model(model, 'rnn_with_attention.h5')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Checking the RNN with attention mechanism"
      ],
      "metadata": {
        "id": "lmcGo3289-Se"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import joblib\n",
        "from tensorflow.keras.models import load_model\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "import numpy as np\n",
        "import re\n",
        "import pandas as pd\n",
        "\n",
        "# Load the label encoder, tokenizer, max_text_length, and scaler\n",
        "label_encoder = joblib.load('label_encoder.pkl')\n",
        "tokenizer = joblib.load('tokenizer.pkl')\n",
        "max_text_length = joblib.load('max_text_length.pkl')\n",
        "scaler = joblib.load('scaler.pkl')\n",
        "\n",
        "# Load the trained model\n",
        "model = load_model('rnn_with_attention.h5', custom_objects={'Attention': Attention})\n",
        "\n",
        "# Load dataset from Kaggle path\n",
        "dataset_path = \"dataset_phishing.csv\"\n",
        "dataset = pd.read_csv(dataset_path)\n",
        "\n",
        "# Function to check URL\n",
        "def check_url(url):\n",
        "    # Check if the URL is in the dataset\n",
        "    if url in dataset['url'].values:\n",
        "        # Get features from the dataset\n",
        "        data_row = dataset[dataset['url'] == url].iloc[0]\n",
        "        ip_present = data_row['ip']\n",
        "        dns_record = data_row['dns_record']\n",
        "        google_index = data_row['google_index']\n",
        "    else:\n",
        "        # Compute features based on the URL\n",
        "        ip_present = 1 if re.search(r'\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}', url) is not None else 0\n",
        "        dns_record = 1 if 'dns' in url else 0\n",
        "        google_index = 1 if 'google' in url else 0\n",
        "\n",
        "    # Tokenize and pad the test URL\n",
        "    test_sequence = tokenizer.texts_to_sequences([url])\n",
        "    test_sequence_padded = pad_sequences(test_sequence, maxlen=max_text_length, padding='post')\n",
        "\n",
        "    # Compute URL length\n",
        "    length_url = len(url)\n",
        "    print(f'URL Length: {length_url}')  # Print URL length\n",
        "\n",
        "    # Compute hostname length\n",
        "    length_hostname = len(url.split('//')[-1].split('/')[0])\n",
        "    print(f'Hostname Length: {length_hostname}')  # Print hostname length\n",
        "\n",
        "    # Print IP address presence\n",
        "    print(f'IP Address Presence: {ip_present}')\n",
        "\n",
        "    # Print DNS record presence\n",
        "    print(f'DNS Record Presence: {dns_record}')\n",
        "\n",
        "    # Print Google index presence\n",
        "    print(f'Google Index Presence: {google_index}')\n",
        "\n",
        "    # Scale the URL length, hostname length, IP address presence, DNS record presence, and Google index presence\n",
        "    features = [[length_url, length_hostname, ip_present, dns_record, google_index]]\n",
        "    scaled_features = scaler.transform(features)\n",
        "\n",
        "    # Make prediction\n",
        "    prediction = model.predict([test_sequence_padded, scaled_features])\n",
        "\n",
        "    # Print prediction\n",
        "    if prediction > 0.5:\n",
        "        return \"The URL is predicted to be phishing.\"\n",
        "    else:\n",
        "        return \"The URL is predicted to be legitimate.\"\n",
        "\n",
        "# Example usage\n",
        "test_url = \"http://latinasonline09.webcindario.com/app/facebook.com/?lang=de\"\n",
        "result = check_url(test_url)\n",
        "print(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "kNyqj3Uhk6hL",
        "outputId": "62ab7eec-a6f1-4d2b-9565-84378ecc2ded"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "URL Length: 64\n",
            "Hostname Length: 31\n",
            "IP Address Presence: 0\n",
            "DNS Record Presence: 0\n",
            "Google Index Presence: 1\n",
            "1/1 [==============================] - 1s 932ms/step\n",
            "The URL is predicted to be phishing.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# R-CNN"
      ],
      "metadata": {
        "id": "GSXE0N_f-F2N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Embedding, Conv1D, Dense, GlobalMaxPooling1D, LSTM, Concatenate\n",
        "from sklearn.metrics import classification_report\n",
        "import numpy as np\n",
        "import joblib\n",
        "from tensorflow.keras.models import save_model\n",
        "\n",
        "# Load dataset from Kaggle path\n",
        "dataset_path = \"dataset_phishing.csv\"\n",
        "dataset = pd.read_csv(dataset_path)\n",
        "\n",
        "# Add URL length as a feature\n",
        "dataset['length_url'] = dataset['url'].apply(len)\n",
        "# Add hostname length as a feature\n",
        "dataset['length_hostname'] = dataset['url'].apply(lambda x: len(x.split('//')[-1].split('/')[0]))\n",
        "\n",
        "# Extract numerical features including IP address presence, DNS record presence, and Google index presence\n",
        "dataset['ip'] = dataset['ip']\n",
        "dataset['dns_record'] = dataset['dns_record']\n",
        "dataset['google_index'] = dataset['google_index']\n",
        "\n",
        "# Encode categorical labels\n",
        "label_encoder = LabelEncoder()\n",
        "dataset['status'] = label_encoder.fit_transform(dataset['status'])\n",
        "\n",
        "# Save the label encoder\n",
        "joblib.dump(label_encoder, 'label_encoder.pkl')\n",
        "\n",
        "# Text preprocessing\n",
        "texts = dataset['url'].values  # Assuming 'url' is the text column\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(texts)\n",
        "sequences = tokenizer.texts_to_sequences(texts)\n",
        "max_text_length = max([len(seq) for seq in sequences])\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "\n",
        "# Save the tokenizer\n",
        "joblib.dump(tokenizer, 'tokenizer.pkl')\n",
        "joblib.dump(max_text_length, 'max_text_length.pkl')\n",
        "\n",
        "# Padding sequences\n",
        "X_text = pad_sequences(sequences, maxlen=max_text_length, padding='post')\n",
        "\n",
        "# Extract numerical features\n",
        "X_num = dataset[['length_url', 'length_hostname', 'ip', 'dns_record', 'google_index']].values\n",
        "scaler = StandardScaler()\n",
        "X_num = scaler.fit_transform(X_num)\n",
        "\n",
        "# Save the scaler\n",
        "joblib.dump(scaler, 'scaler.pkl')\n",
        "\n",
        "# Split features and labels\n",
        "y = dataset['status']\n",
        "X_train_text, X_test_text, X_train_num, X_test_num, y_train, y_test = train_test_split(\n",
        "    X_text, X_num, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Define CNN model for textual features\n",
        "input_text = Input(shape=(max_text_length,))\n",
        "embedding_layer = Embedding(vocab_size, 100)(input_text)\n",
        "conv_layer = Conv1D(filters=128, kernel_size=5, activation='relu')(embedding_layer)\n",
        "pooling_layer = GlobalMaxPooling1D()(conv_layer)\n",
        "\n",
        "# Define LSTM layer for textual features\n",
        "lstm_layer = LSTM(64)(embedding_layer)\n",
        "\n",
        "# Define dense layer for numerical features\n",
        "input_num = Input(shape=(X_train_num.shape[1],))\n",
        "dense_layer = Dense(64, activation='relu')(input_num)\n",
        "\n",
        "# Concatenate text and numerical features\n",
        "concatenated = Concatenate()([pooling_layer, lstm_layer, dense_layer])\n",
        "output = Dense(1, activation='sigmoid')(concatenated)\n",
        "\n",
        "# Define combined model\n",
        "model = Model(inputs=[input_text, input_num], outputs=output)\n",
        "\n",
        "# Compile model\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train model\n",
        "model.fit([X_train_text, X_train_num], y_train, epochs=100, batch_size=64, validation_split=0.1)\n",
        "\n",
        "# Evaluate model\n",
        "loss, accuracy = model.evaluate([X_test_text, X_test_num], y_test)\n",
        "print(f'Test Accuracy: {accuracy}')\n",
        "\n",
        "# Generate predictions for test set\n",
        "y_pred_probs = model.predict([X_test_text, X_test_num])\n",
        "y_pred = (y_pred_probs > 0.5).astype(int)\n",
        "\n",
        "# Print classification report\n",
        "print(classification_report(y_test, y_pred))\n",
        "\n",
        "# Save the trained model\n",
        "save_model(model, 'rcnn.h5')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "hsvL40sHr5GV",
        "outputId": "5e3ad1eb-d434-41f3-e96f-8fc668487357"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "129/129 [==============================] - 32s 225ms/step - loss: 0.3550 - accuracy: 0.8645 - val_loss: 0.1802 - val_accuracy: 0.9355\n",
            "Epoch 2/100\n",
            "129/129 [==============================] - 28s 220ms/step - loss: 0.0798 - accuracy: 0.9765 - val_loss: 0.1381 - val_accuracy: 0.9508\n",
            "Epoch 3/100\n",
            "129/129 [==============================] - 29s 224ms/step - loss: 0.0140 - accuracy: 0.9972 - val_loss: 0.1415 - val_accuracy: 0.9530\n",
            "Epoch 4/100\n",
            "129/129 [==============================] - 29s 224ms/step - loss: 0.0028 - accuracy: 0.9998 - val_loss: 0.1389 - val_accuracy: 0.9530\n",
            "Epoch 5/100\n",
            "129/129 [==============================] - 29s 224ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 0.1478 - val_accuracy: 0.9497\n",
            "Epoch 6/100\n",
            "129/129 [==============================] - 28s 215ms/step - loss: 6.0739e-04 - accuracy: 1.0000 - val_loss: 0.1509 - val_accuracy: 0.9497\n",
            "Epoch 7/100\n",
            "129/129 [==============================] - 29s 221ms/step - loss: 4.0292e-04 - accuracy: 1.0000 - val_loss: 0.1526 - val_accuracy: 0.9486\n",
            "Epoch 8/100\n",
            "129/129 [==============================] - 28s 217ms/step - loss: 2.8847e-04 - accuracy: 1.0000 - val_loss: 0.1568 - val_accuracy: 0.9497\n",
            "Epoch 9/100\n",
            "129/129 [==============================] - 28s 220ms/step - loss: 2.1403e-04 - accuracy: 1.0000 - val_loss: 0.1582 - val_accuracy: 0.9486\n",
            "Epoch 10/100\n",
            "129/129 [==============================] - 28s 219ms/step - loss: 1.6615e-04 - accuracy: 1.0000 - val_loss: 0.1607 - val_accuracy: 0.9486\n",
            "Epoch 11/100\n",
            "129/129 [==============================] - 29s 221ms/step - loss: 1.3192e-04 - accuracy: 1.0000 - val_loss: 0.1625 - val_accuracy: 0.9497\n",
            "Epoch 12/100\n",
            "129/129 [==============================] - 28s 219ms/step - loss: 1.0710e-04 - accuracy: 1.0000 - val_loss: 0.1651 - val_accuracy: 0.9486\n",
            "Epoch 13/100\n",
            "129/129 [==============================] - 28s 218ms/step - loss: 8.8442e-05 - accuracy: 1.0000 - val_loss: 0.1666 - val_accuracy: 0.9486\n",
            "Epoch 14/100\n",
            "129/129 [==============================] - 29s 221ms/step - loss: 7.3974e-05 - accuracy: 1.0000 - val_loss: 0.1680 - val_accuracy: 0.9486\n",
            "Epoch 15/100\n",
            "129/129 [==============================] - 28s 218ms/step - loss: 6.2493e-05 - accuracy: 1.0000 - val_loss: 0.1696 - val_accuracy: 0.9486\n",
            "Epoch 16/100\n",
            "129/129 [==============================] - 28s 220ms/step - loss: 5.3356e-05 - accuracy: 1.0000 - val_loss: 0.1713 - val_accuracy: 0.9497\n",
            "Epoch 17/100\n",
            "129/129 [==============================] - 28s 218ms/step - loss: 4.5915e-05 - accuracy: 1.0000 - val_loss: 0.1727 - val_accuracy: 0.9486\n",
            "Epoch 18/100\n",
            "129/129 [==============================] - 28s 216ms/step - loss: 3.9856e-05 - accuracy: 1.0000 - val_loss: 0.1741 - val_accuracy: 0.9497\n",
            "Epoch 19/100\n",
            "129/129 [==============================] - 28s 216ms/step - loss: 3.4753e-05 - accuracy: 1.0000 - val_loss: 0.1755 - val_accuracy: 0.9486\n",
            "Epoch 20/100\n",
            "129/129 [==============================] - 28s 218ms/step - loss: 3.0488e-05 - accuracy: 1.0000 - val_loss: 0.1767 - val_accuracy: 0.9486\n",
            "Epoch 21/100\n",
            "129/129 [==============================] - 28s 217ms/step - loss: 2.6900e-05 - accuracy: 1.0000 - val_loss: 0.1783 - val_accuracy: 0.9486\n",
            "Epoch 22/100\n",
            "129/129 [==============================] - 28s 220ms/step - loss: 2.3808e-05 - accuracy: 1.0000 - val_loss: 0.1793 - val_accuracy: 0.9486\n",
            "Epoch 23/100\n",
            "129/129 [==============================] - 28s 219ms/step - loss: 2.1185e-05 - accuracy: 1.0000 - val_loss: 0.1806 - val_accuracy: 0.9486\n",
            "Epoch 24/100\n",
            "129/129 [==============================] - 28s 218ms/step - loss: 1.8889e-05 - accuracy: 1.0000 - val_loss: 0.1817 - val_accuracy: 0.9486\n",
            "Epoch 25/100\n",
            "129/129 [==============================] - 28s 218ms/step - loss: 1.6906e-05 - accuracy: 1.0000 - val_loss: 0.1830 - val_accuracy: 0.9486\n",
            "Epoch 26/100\n",
            "129/129 [==============================] - 28s 219ms/step - loss: 1.5184e-05 - accuracy: 1.0000 - val_loss: 0.1841 - val_accuracy: 0.9486\n",
            "Epoch 27/100\n",
            "129/129 [==============================] - 28s 218ms/step - loss: 1.3661e-05 - accuracy: 1.0000 - val_loss: 0.1852 - val_accuracy: 0.9486\n",
            "Epoch 28/100\n",
            "129/129 [==============================] - 28s 218ms/step - loss: 1.2328e-05 - accuracy: 1.0000 - val_loss: 0.1864 - val_accuracy: 0.9486\n",
            "Epoch 29/100\n",
            "129/129 [==============================] - 29s 220ms/step - loss: 1.1148e-05 - accuracy: 1.0000 - val_loss: 0.1876 - val_accuracy: 0.9475\n",
            "Epoch 30/100\n",
            "129/129 [==============================] - 29s 221ms/step - loss: 1.0087e-05 - accuracy: 1.0000 - val_loss: 0.1885 - val_accuracy: 0.9475\n",
            "Epoch 31/100\n",
            "129/129 [==============================] - 29s 223ms/step - loss: 9.1606e-06 - accuracy: 1.0000 - val_loss: 0.1897 - val_accuracy: 0.9475\n",
            "Epoch 32/100\n",
            "129/129 [==============================] - 29s 224ms/step - loss: 8.3298e-06 - accuracy: 1.0000 - val_loss: 0.1907 - val_accuracy: 0.9475\n",
            "Epoch 33/100\n",
            "129/129 [==============================] - 29s 224ms/step - loss: 7.5752e-06 - accuracy: 1.0000 - val_loss: 0.1918 - val_accuracy: 0.9475\n",
            "Epoch 34/100\n",
            "129/129 [==============================] - 29s 225ms/step - loss: 6.9065e-06 - accuracy: 1.0000 - val_loss: 0.1928 - val_accuracy: 0.9475\n",
            "Epoch 35/100\n",
            "129/129 [==============================] - 28s 219ms/step - loss: 6.3092e-06 - accuracy: 1.0000 - val_loss: 0.1938 - val_accuracy: 0.9486\n",
            "Epoch 36/100\n",
            "129/129 [==============================] - 29s 226ms/step - loss: 5.7633e-06 - accuracy: 1.0000 - val_loss: 0.1948 - val_accuracy: 0.9475\n",
            "Epoch 37/100\n",
            "129/129 [==============================] - 29s 223ms/step - loss: 5.2722e-06 - accuracy: 1.0000 - val_loss: 0.1958 - val_accuracy: 0.9486\n",
            "Epoch 38/100\n",
            "129/129 [==============================] - 29s 226ms/step - loss: 4.8307e-06 - accuracy: 1.0000 - val_loss: 0.1968 - val_accuracy: 0.9475\n",
            "Epoch 39/100\n",
            "129/129 [==============================] - 28s 220ms/step - loss: 4.4286e-06 - accuracy: 1.0000 - val_loss: 0.1978 - val_accuracy: 0.9486\n",
            "Epoch 40/100\n",
            "129/129 [==============================] - 29s 225ms/step - loss: 4.0667e-06 - accuracy: 1.0000 - val_loss: 0.1987 - val_accuracy: 0.9475\n",
            "Epoch 41/100\n",
            "129/129 [==============================] - 29s 226ms/step - loss: 3.7354e-06 - accuracy: 1.0000 - val_loss: 0.1998 - val_accuracy: 0.9475\n",
            "Epoch 42/100\n",
            "129/129 [==============================] - 29s 225ms/step - loss: 3.4320e-06 - accuracy: 1.0000 - val_loss: 0.2007 - val_accuracy: 0.9475\n",
            "Epoch 43/100\n",
            "129/129 [==============================] - 29s 222ms/step - loss: 3.1577e-06 - accuracy: 1.0000 - val_loss: 0.2017 - val_accuracy: 0.9475\n",
            "Epoch 44/100\n",
            "129/129 [==============================] - 29s 223ms/step - loss: 2.9060e-06 - accuracy: 1.0000 - val_loss: 0.2027 - val_accuracy: 0.9464\n",
            "Epoch 45/100\n",
            "129/129 [==============================] - 29s 226ms/step - loss: 2.6795e-06 - accuracy: 1.0000 - val_loss: 0.2036 - val_accuracy: 0.9464\n",
            "Epoch 46/100\n",
            "129/129 [==============================] - 29s 225ms/step - loss: 2.4690e-06 - accuracy: 1.0000 - val_loss: 0.2046 - val_accuracy: 0.9464\n",
            "Epoch 47/100\n",
            "129/129 [==============================] - 29s 223ms/step - loss: 2.2775e-06 - accuracy: 1.0000 - val_loss: 0.2055 - val_accuracy: 0.9475\n",
            "Epoch 48/100\n",
            "129/129 [==============================] - 29s 227ms/step - loss: 2.1029e-06 - accuracy: 1.0000 - val_loss: 0.2065 - val_accuracy: 0.9475\n",
            "Epoch 49/100\n",
            "129/129 [==============================] - 29s 225ms/step - loss: 1.9416e-06 - accuracy: 1.0000 - val_loss: 0.2074 - val_accuracy: 0.9475\n",
            "Epoch 50/100\n",
            "129/129 [==============================] - 29s 222ms/step - loss: 1.7941e-06 - accuracy: 1.0000 - val_loss: 0.2084 - val_accuracy: 0.9475\n",
            "Epoch 51/100\n",
            "129/129 [==============================] - 29s 223ms/step - loss: 1.6567e-06 - accuracy: 1.0000 - val_loss: 0.2094 - val_accuracy: 0.9475\n",
            "Epoch 52/100\n",
            "129/129 [==============================] - 28s 219ms/step - loss: 1.5344e-06 - accuracy: 1.0000 - val_loss: 0.2103 - val_accuracy: 0.9475\n",
            "Epoch 53/100\n",
            "129/129 [==============================] - 28s 220ms/step - loss: 1.4184e-06 - accuracy: 1.0000 - val_loss: 0.2112 - val_accuracy: 0.9475\n",
            "Epoch 54/100\n",
            "129/129 [==============================] - 29s 223ms/step - loss: 1.3149e-06 - accuracy: 1.0000 - val_loss: 0.2120 - val_accuracy: 0.9475\n",
            "Epoch 55/100\n",
            "129/129 [==============================] - 28s 219ms/step - loss: 1.2173e-06 - accuracy: 1.0000 - val_loss: 0.2130 - val_accuracy: 0.9475\n",
            "Epoch 56/100\n",
            "129/129 [==============================] - 28s 217ms/step - loss: 1.1270e-06 - accuracy: 1.0000 - val_loss: 0.2140 - val_accuracy: 0.9475\n",
            "Epoch 57/100\n",
            "129/129 [==============================] - 28s 215ms/step - loss: 1.0451e-06 - accuracy: 1.0000 - val_loss: 0.2149 - val_accuracy: 0.9475\n",
            "Epoch 58/100\n",
            "129/129 [==============================] - 28s 215ms/step - loss: 9.6853e-07 - accuracy: 1.0000 - val_loss: 0.2158 - val_accuracy: 0.9475\n",
            "Epoch 59/100\n",
            "129/129 [==============================] - 29s 222ms/step - loss: 8.9823e-07 - accuracy: 1.0000 - val_loss: 0.2168 - val_accuracy: 0.9475\n",
            "Epoch 60/100\n",
            "129/129 [==============================] - 28s 220ms/step - loss: 8.3301e-07 - accuracy: 1.0000 - val_loss: 0.2177 - val_accuracy: 0.9475\n",
            "Epoch 61/100\n",
            "129/129 [==============================] - 28s 219ms/step - loss: 7.7374e-07 - accuracy: 1.0000 - val_loss: 0.2186 - val_accuracy: 0.9475\n",
            "Epoch 62/100\n",
            "129/129 [==============================] - 29s 221ms/step - loss: 7.1816e-07 - accuracy: 1.0000 - val_loss: 0.2195 - val_accuracy: 0.9475\n",
            "Epoch 63/100\n",
            "129/129 [==============================] - 29s 224ms/step - loss: 6.6738e-07 - accuracy: 1.0000 - val_loss: 0.2204 - val_accuracy: 0.9464\n",
            "Epoch 64/100\n",
            "129/129 [==============================] - 29s 226ms/step - loss: 6.1972e-07 - accuracy: 1.0000 - val_loss: 0.2212 - val_accuracy: 0.9464\n",
            "Epoch 65/100\n",
            "129/129 [==============================] - 28s 218ms/step - loss: 5.7600e-07 - accuracy: 1.0000 - val_loss: 0.2221 - val_accuracy: 0.9464\n",
            "Epoch 66/100\n",
            "129/129 [==============================] - 28s 220ms/step - loss: 5.3569e-07 - accuracy: 1.0000 - val_loss: 0.2231 - val_accuracy: 0.9464\n",
            "Epoch 67/100\n",
            "129/129 [==============================] - 29s 227ms/step - loss: 4.9821e-07 - accuracy: 1.0000 - val_loss: 0.2239 - val_accuracy: 0.9454\n",
            "Epoch 68/100\n",
            "129/129 [==============================] - 29s 228ms/step - loss: 4.6368e-07 - accuracy: 1.0000 - val_loss: 0.2248 - val_accuracy: 0.9454\n",
            "Epoch 69/100\n",
            "129/129 [==============================] - 29s 225ms/step - loss: 4.3169e-07 - accuracy: 1.0000 - val_loss: 0.2258 - val_accuracy: 0.9454\n",
            "Epoch 70/100\n",
            "129/129 [==============================] - 29s 227ms/step - loss: 4.0160e-07 - accuracy: 1.0000 - val_loss: 0.2267 - val_accuracy: 0.9454\n",
            "Epoch 71/100\n",
            "129/129 [==============================] - 29s 226ms/step - loss: 3.7392e-07 - accuracy: 1.0000 - val_loss: 0.2276 - val_accuracy: 0.9443\n",
            "Epoch 72/100\n",
            "129/129 [==============================] - 29s 226ms/step - loss: 3.4825e-07 - accuracy: 1.0000 - val_loss: 0.2285 - val_accuracy: 0.9443\n",
            "Epoch 73/100\n",
            "129/129 [==============================] - 29s 224ms/step - loss: 3.2457e-07 - accuracy: 1.0000 - val_loss: 0.2294 - val_accuracy: 0.9443\n",
            "Epoch 74/100\n",
            "129/129 [==============================] - 29s 225ms/step - loss: 3.0259e-07 - accuracy: 1.0000 - val_loss: 0.2303 - val_accuracy: 0.9443\n",
            "Epoch 75/100\n",
            "129/129 [==============================] - 29s 226ms/step - loss: 2.8217e-07 - accuracy: 1.0000 - val_loss: 0.2311 - val_accuracy: 0.9443\n",
            "Epoch 76/100\n",
            "129/129 [==============================] - 29s 224ms/step - loss: 2.6342e-07 - accuracy: 1.0000 - val_loss: 0.2320 - val_accuracy: 0.9443\n",
            "Epoch 77/100\n",
            "129/129 [==============================] - 29s 223ms/step - loss: 2.4568e-07 - accuracy: 1.0000 - val_loss: 0.2330 - val_accuracy: 0.9443\n",
            "Epoch 78/100\n",
            "129/129 [==============================] - 29s 222ms/step - loss: 2.2923e-07 - accuracy: 1.0000 - val_loss: 0.2338 - val_accuracy: 0.9443\n",
            "Epoch 79/100\n",
            "129/129 [==============================] - 28s 216ms/step - loss: 2.1406e-07 - accuracy: 1.0000 - val_loss: 0.2347 - val_accuracy: 0.9432\n",
            "Epoch 80/100\n",
            "129/129 [==============================] - 29s 223ms/step - loss: 2.0002e-07 - accuracy: 1.0000 - val_loss: 0.2356 - val_accuracy: 0.9432\n",
            "Epoch 81/100\n",
            "129/129 [==============================] - 28s 218ms/step - loss: 1.8685e-07 - accuracy: 1.0000 - val_loss: 0.2364 - val_accuracy: 0.9432\n",
            "Epoch 82/100\n",
            "129/129 [==============================] - 29s 223ms/step - loss: 1.7470e-07 - accuracy: 1.0000 - val_loss: 0.2373 - val_accuracy: 0.9432\n",
            "Epoch 83/100\n",
            "129/129 [==============================] - 28s 219ms/step - loss: 1.6344e-07 - accuracy: 1.0000 - val_loss: 0.2382 - val_accuracy: 0.9432\n",
            "Epoch 84/100\n",
            "129/129 [==============================] - 28s 217ms/step - loss: 1.5272e-07 - accuracy: 1.0000 - val_loss: 0.2390 - val_accuracy: 0.9432\n",
            "Epoch 85/100\n",
            "129/129 [==============================] - 28s 217ms/step - loss: 1.4298e-07 - accuracy: 1.0000 - val_loss: 0.2399 - val_accuracy: 0.9432\n",
            "Epoch 86/100\n",
            "129/129 [==============================] - 28s 214ms/step - loss: 1.3385e-07 - accuracy: 1.0000 - val_loss: 0.2407 - val_accuracy: 0.9432\n",
            "Epoch 87/100\n",
            "129/129 [==============================] - 28s 218ms/step - loss: 1.2536e-07 - accuracy: 1.0000 - val_loss: 0.2415 - val_accuracy: 0.9421\n",
            "Epoch 88/100\n",
            "129/129 [==============================] - 28s 215ms/step - loss: 1.1747e-07 - accuracy: 1.0000 - val_loss: 0.2424 - val_accuracy: 0.9432\n",
            "Epoch 89/100\n",
            "129/129 [==============================] - 28s 216ms/step - loss: 1.1000e-07 - accuracy: 1.0000 - val_loss: 0.2433 - val_accuracy: 0.9432\n",
            "Epoch 90/100\n",
            "129/129 [==============================] - 28s 219ms/step - loss: 1.0320e-07 - accuracy: 1.0000 - val_loss: 0.2441 - val_accuracy: 0.9432\n",
            "Epoch 91/100\n",
            "129/129 [==============================] - 28s 219ms/step - loss: 9.6705e-08 - accuracy: 1.0000 - val_loss: 0.2449 - val_accuracy: 0.9432\n",
            "Epoch 92/100\n",
            "129/129 [==============================] - 31s 236ms/step - loss: 9.0725e-08 - accuracy: 1.0000 - val_loss: 0.2458 - val_accuracy: 0.9432\n",
            "Epoch 93/100\n",
            "129/129 [==============================] - 32s 249ms/step - loss: 8.5118e-08 - accuracy: 1.0000 - val_loss: 0.2466 - val_accuracy: 0.9432\n",
            "Epoch 94/100\n",
            "129/129 [==============================] - 32s 252ms/step - loss: 7.9967e-08 - accuracy: 1.0000 - val_loss: 0.2475 - val_accuracy: 0.9432\n",
            "Epoch 95/100\n",
            "129/129 [==============================] - 29s 229ms/step - loss: 7.5085e-08 - accuracy: 1.0000 - val_loss: 0.2483 - val_accuracy: 0.9421\n",
            "Epoch 96/100\n",
            "129/129 [==============================] - 29s 223ms/step - loss: 7.0506e-08 - accuracy: 1.0000 - val_loss: 0.2492 - val_accuracy: 0.9421\n",
            "Epoch 97/100\n",
            "129/129 [==============================] - 29s 229ms/step - loss: 6.6254e-08 - accuracy: 1.0000 - val_loss: 0.2499 - val_accuracy: 0.9410\n",
            "Epoch 98/100\n",
            "129/129 [==============================] - 29s 225ms/step - loss: 6.2329e-08 - accuracy: 1.0000 - val_loss: 0.2508 - val_accuracy: 0.9410\n",
            "Epoch 99/100\n",
            "129/129 [==============================] - 29s 227ms/step - loss: 5.8614e-08 - accuracy: 1.0000 - val_loss: 0.2516 - val_accuracy: 0.9410\n",
            "Epoch 100/100\n",
            "129/129 [==============================] - 29s 222ms/step - loss: 5.5169e-08 - accuracy: 1.0000 - val_loss: 0.2524 - val_accuracy: 0.9399\n",
            "72/72 [==============================] - 3s 43ms/step - loss: 0.2537 - accuracy: 0.9388\n",
            "Test Accuracy: 0.9387576580047607\n",
            "72/72 [==============================] - 3s 29ms/step\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.93      0.95      0.94      1157\n",
            "           1       0.95      0.93      0.94      1129\n",
            "\n",
            "    accuracy                           0.94      2286\n",
            "   macro avg       0.94      0.94      0.94      2286\n",
            "weighted avg       0.94      0.94      0.94      2286\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-23-c8ffd0771992>:102: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  save_model(model, 'rcnn.h5')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Checking the R-CNN model"
      ],
      "metadata": {
        "id": "xIzhQTuf-Iku"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import joblib\n",
        "from tensorflow.keras.models import load_model\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "import numpy as np\n",
        "import re\n",
        "import pandas as pd\n",
        "\n",
        "# Load the label encoder, tokenizer, max_text_length, and scaler\n",
        "label_encoder = joblib.load('label_encoder.pkl')\n",
        "tokenizer = joblib.load('tokenizer.pkl')\n",
        "max_text_length = joblib.load('max_text_length.pkl')\n",
        "scaler = joblib.load('scaler.pkl')\n",
        "\n",
        "# Load the trained model\n",
        "model = load_model('rcnn.h5')  # Load the R-CNN model\n",
        "\n",
        "# Load dataset from Kaggle path\n",
        "dataset_path = \"dataset_phishing.csv\"\n",
        "dataset = pd.read_csv(dataset_path)\n",
        "\n",
        "# Function to check URL\n",
        "def check_url(url):\n",
        "    # Check if the URL is in the dataset\n",
        "    if url in dataset['url'].values:\n",
        "        # Get features from the dataset\n",
        "        data_row = dataset[dataset['url'] == url].iloc[0]\n",
        "        ip_present = data_row['ip']\n",
        "        dns_record = data_row['dns_record']\n",
        "        google_index = data_row['google_index']\n",
        "    else:\n",
        "        # Compute features based on the URL\n",
        "        ip_present = 1 if re.search(r'\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}', url) is not None else 0\n",
        "        dns_record = 1 if 'dns' in url else 0\n",
        "        google_index = 1 if 'google' in url else 0\n",
        "\n",
        "    # Tokenize and pad the test URL\n",
        "    test_sequence = tokenizer.texts_to_sequences([url])\n",
        "    test_sequence_padded = pad_sequences(test_sequence, maxlen=max_text_length, padding='post')\n",
        "\n",
        "    # Compute URL length\n",
        "    length_url = len(url)\n",
        "    print(f'URL Length: {length_url}')  # Print URL length\n",
        "\n",
        "    # Compute hostname length\n",
        "    length_hostname = len(url.split('//')[-1].split('/')[0])\n",
        "    print(f'Hostname Length: {length_hostname}')  # Print hostname length\n",
        "\n",
        "    # Print IP address presence\n",
        "    print(f'IP Address Presence: {ip_present}')\n",
        "\n",
        "    # Print DNS record presence\n",
        "    print(f'DNS Record Presence: {dns_record}')\n",
        "\n",
        "    # Print Google index presence\n",
        "    print(f'Google Index Presence: {google_index}')\n",
        "\n",
        "    # Scale the URL length, hostname length, IP address presence, DNS record presence, and Google index presence\n",
        "    features = [[length_url, length_hostname, ip_present, dns_record, google_index]]\n",
        "    scaled_features = scaler.transform(features)\n",
        "\n",
        "    # Make prediction\n",
        "    prediction = model.predict([test_sequence_padded, scaled_features])\n",
        "\n",
        "    # Print prediction\n",
        "    if prediction > 0.5:\n",
        "        return \"The URL is predicted to be phishing.\"\n",
        "    else:\n",
        "        return \"The URL is predicted to be legitimate.\"\n",
        "\n",
        "# Example usage\n",
        "test_url = \"https://www.corbaraweb.com/shop/it/\"\n",
        "result = check_url(test_url)\n",
        "print(result)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "VOB7oKQ8sCre",
        "outputId": "6d6f85a4-a9d5-404d-c610-ac8e7cd0040a"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "URL Length: 35\n",
            "Hostname Length: 18\n",
            "IP Address Presence: 0\n",
            "DNS Record Presence: 0\n",
            "Google Index Presence: 0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:5 out of the last 76 calls to <function Model.make_predict_function.<locals>.predict_function at 0x78ba7167b520> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 1s 874ms/step\n",
            "The URL is predicted to be legitimate.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LSTM"
      ],
      "metadata": {
        "id": "CVYM50cl-MjZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Embedding, LSTM, Dense, Concatenate\n",
        "from sklearn.metrics import classification_report\n",
        "import numpy as np\n",
        "import joblib\n",
        "from tensorflow.keras.models import save_model\n",
        "\n",
        "# Load dataset from Kaggle path\n",
        "dataset_path = \"dataset_phishing.csv\"\n",
        "dataset = pd.read_csv(dataset_path)\n",
        "\n",
        "# Add URL length as a feature\n",
        "dataset['length_url'] = dataset['url'].apply(len)\n",
        "# Add hostname length as a feature\n",
        "dataset['length_hostname'] = dataset['url'].apply(lambda x: len(x.split('//')[-1].split('/')[0]))\n",
        "\n",
        "# Extract numerical features including IP address presence, DNS record presence, and Google index presence\n",
        "dataset['ip'] = dataset['ip']\n",
        "dataset['dns_record'] = dataset['dns_record']\n",
        "dataset['google_index'] = dataset['google_index']\n",
        "\n",
        "# Encode categorical labels\n",
        "label_encoder = LabelEncoder()\n",
        "dataset['status'] = label_encoder.fit_transform(dataset['status'])\n",
        "\n",
        "# Save the label encoder\n",
        "joblib.dump(label_encoder, 'label_encoder.pkl')\n",
        "\n",
        "# Text preprocessing\n",
        "texts = dataset['url'].values  # Assuming 'url' is the text column\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(texts)\n",
        "sequences = tokenizer.texts_to_sequences(texts)\n",
        "max_text_length = max([len(seq) for seq in sequences])\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "\n",
        "# Save the tokenizer\n",
        "joblib.dump(tokenizer, 'tokenizer.pkl')\n",
        "joblib.dump(max_text_length, 'max_text_length.pkl')\n",
        "\n",
        "# Padding sequences\n",
        "X_text = pad_sequences(sequences, maxlen=max_text_length, padding='post')\n",
        "\n",
        "# Extract numerical features\n",
        "X_num = dataset[['length_url', 'length_hostname', 'ip', 'dns_record', 'google_index']].values\n",
        "scaler = StandardScaler()\n",
        "X_num = scaler.fit_transform(X_num)\n",
        "\n",
        "# Save the scaler\n",
        "joblib.dump(scaler, 'scaler.pkl')\n",
        "\n",
        "# Split features and labels\n",
        "y = dataset['status']\n",
        "X_train_text, X_test_text, X_train_num, X_test_num, y_train, y_test = train_test_split(\n",
        "    X_text, X_num, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Define LSTM model for textual features\n",
        "input_text = Input(shape=(max_text_length,))\n",
        "embedding_layer = Embedding(vocab_size, 100)(input_text)\n",
        "lstm_layer = LSTM(128)(embedding_layer)\n",
        "\n",
        "# Define dense layer for numerical features\n",
        "input_num = Input(shape=(X_train_num.shape[1],))\n",
        "dense_layer = Dense(64, activation='relu')(input_num)\n",
        "\n",
        "# Concatenate text and numerical features\n",
        "concatenated = Concatenate()([lstm_layer, dense_layer])\n",
        "output = Dense(1, activation='sigmoid')(concatenated)\n",
        "\n",
        "# Define combined model\n",
        "model = Model(inputs=[input_text, input_num], outputs=output)\n",
        "\n",
        "# Compile model\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train model\n",
        "model.fit([X_train_text, X_train_num], y_train, epochs=100, batch_size=64, validation_split=0.1)\n",
        "\n",
        "# Evaluate model\n",
        "loss, accuracy = model.evaluate([X_test_text, X_test_num], y_test)\n",
        "print(f'Test Accuracy: {accuracy}')\n",
        "\n",
        "# Generate predictions for test set\n",
        "y_pred_probs = model.predict([X_test_text, X_test_num])\n",
        "y_pred = (y_pred_probs > 0.5).astype(int)\n",
        "\n",
        "# Print classification report\n",
        "print(classification_report(y_test, y_pred))\n",
        "\n",
        "# Save the trained model\n",
        "save_model(model, 'lstm.h5')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "gzw4YMTVsOKn",
        "outputId": "3ff8b516-9ac3-41ce-c1c5-3c74898d2224"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "129/129 [==============================] - 43s 317ms/step - loss: 0.4665 - accuracy: 0.8463 - val_loss: 0.3692 - val_accuracy: 0.8557\n",
            "Epoch 2/100\n",
            "129/129 [==============================] - 43s 334ms/step - loss: 0.3656 - accuracy: 0.8663 - val_loss: 0.3576 - val_accuracy: 0.8667\n",
            "Epoch 3/100\n",
            "129/129 [==============================] - 39s 306ms/step - loss: 0.3602 - accuracy: 0.8667 - val_loss: 0.3545 - val_accuracy: 0.8667\n",
            "Epoch 4/100\n",
            "129/129 [==============================] - 39s 306ms/step - loss: 0.3581 - accuracy: 0.8677 - val_loss: 0.3543 - val_accuracy: 0.8678\n",
            "Epoch 5/100\n",
            "129/129 [==============================] - 38s 295ms/step - loss: 0.3561 - accuracy: 0.8678 - val_loss: 0.3515 - val_accuracy: 0.8678\n",
            "Epoch 6/100\n",
            "129/129 [==============================] - 37s 283ms/step - loss: 0.3548 - accuracy: 0.8683 - val_loss: 0.3533 - val_accuracy: 0.8678\n",
            "Epoch 7/100\n",
            "129/129 [==============================] - 38s 297ms/step - loss: 0.3534 - accuracy: 0.8679 - val_loss: 0.3513 - val_accuracy: 0.8678\n",
            "Epoch 8/100\n",
            "129/129 [==============================] - 37s 290ms/step - loss: 0.3525 - accuracy: 0.8688 - val_loss: 0.3517 - val_accuracy: 0.8678\n",
            "Epoch 9/100\n",
            "129/129 [==============================] - 36s 281ms/step - loss: 0.3513 - accuracy: 0.8685 - val_loss: 0.3494 - val_accuracy: 0.8678\n",
            "Epoch 10/100\n",
            "129/129 [==============================] - 38s 293ms/step - loss: 0.3503 - accuracy: 0.8688 - val_loss: 0.3501 - val_accuracy: 0.8678\n",
            "Epoch 11/100\n",
            "129/129 [==============================] - 37s 289ms/step - loss: 0.3492 - accuracy: 0.8694 - val_loss: 0.3523 - val_accuracy: 0.8678\n",
            "Epoch 12/100\n",
            "129/129 [==============================] - 36s 281ms/step - loss: 0.3489 - accuracy: 0.8690 - val_loss: 0.3482 - val_accuracy: 0.8678\n",
            "Epoch 13/100\n",
            "129/129 [==============================] - 37s 289ms/step - loss: 0.3479 - accuracy: 0.8688 - val_loss: 0.3471 - val_accuracy: 0.8678\n",
            "Epoch 14/100\n",
            "129/129 [==============================] - 36s 279ms/step - loss: 0.3475 - accuracy: 0.8695 - val_loss: 0.3515 - val_accuracy: 0.8678\n",
            "Epoch 15/100\n",
            "129/129 [==============================] - 37s 289ms/step - loss: 0.3472 - accuracy: 0.8691 - val_loss: 0.3467 - val_accuracy: 0.8678\n",
            "Epoch 16/100\n",
            "129/129 [==============================] - 38s 291ms/step - loss: 0.3462 - accuracy: 0.8691 - val_loss: 0.3462 - val_accuracy: 0.8678\n",
            "Epoch 17/100\n",
            "129/129 [==============================] - 36s 278ms/step - loss: 0.3460 - accuracy: 0.8694 - val_loss: 0.3467 - val_accuracy: 0.8678\n",
            "Epoch 18/100\n",
            "129/129 [==============================] - 38s 292ms/step - loss: 0.3456 - accuracy: 0.8694 - val_loss: 0.3478 - val_accuracy: 0.8678\n",
            "Epoch 19/100\n",
            "129/129 [==============================] - 37s 289ms/step - loss: 0.3450 - accuracy: 0.8694 - val_loss: 0.3464 - val_accuracy: 0.8678\n",
            "Epoch 20/100\n",
            "129/129 [==============================] - 36s 278ms/step - loss: 0.3447 - accuracy: 0.8691 - val_loss: 0.3476 - val_accuracy: 0.8678\n",
            "Epoch 21/100\n",
            "129/129 [==============================] - 38s 294ms/step - loss: 0.3442 - accuracy: 0.8692 - val_loss: 0.3485 - val_accuracy: 0.8678\n",
            "Epoch 22/100\n",
            "129/129 [==============================] - 42s 325ms/step - loss: 0.3446 - accuracy: 0.8692 - val_loss: 0.3466 - val_accuracy: 0.8678\n",
            "Epoch 23/100\n",
            "129/129 [==============================] - 41s 320ms/step - loss: 0.3436 - accuracy: 0.8691 - val_loss: 0.3476 - val_accuracy: 0.8699\n",
            "Epoch 24/100\n",
            "129/129 [==============================] - 42s 328ms/step - loss: 0.3437 - accuracy: 0.8692 - val_loss: 0.3459 - val_accuracy: 0.8678\n",
            "Epoch 25/100\n",
            "129/129 [==============================] - 42s 325ms/step - loss: 0.3432 - accuracy: 0.8691 - val_loss: 0.3474 - val_accuracy: 0.8678\n",
            "Epoch 26/100\n",
            "129/129 [==============================] - 42s 328ms/step - loss: 0.3433 - accuracy: 0.8692 - val_loss: 0.3457 - val_accuracy: 0.8678\n",
            "Epoch 27/100\n",
            "129/129 [==============================] - 46s 355ms/step - loss: 0.3432 - accuracy: 0.8695 - val_loss: 0.3472 - val_accuracy: 0.8699\n",
            "Epoch 28/100\n",
            "129/129 [==============================] - 45s 349ms/step - loss: 0.3429 - accuracy: 0.8691 - val_loss: 0.3460 - val_accuracy: 0.8678\n",
            "Epoch 29/100\n",
            "129/129 [==============================] - 44s 343ms/step - loss: 0.3428 - accuracy: 0.8692 - val_loss: 0.3454 - val_accuracy: 0.8678\n",
            "Epoch 30/100\n",
            "129/129 [==============================] - 44s 345ms/step - loss: 0.3422 - accuracy: 0.8696 - val_loss: 0.3455 - val_accuracy: 0.8678\n",
            "Epoch 31/100\n",
            "129/129 [==============================] - 43s 333ms/step - loss: 0.3421 - accuracy: 0.8692 - val_loss: 0.3459 - val_accuracy: 0.8678\n",
            "Epoch 32/100\n",
            "129/129 [==============================] - 42s 328ms/step - loss: 0.3420 - accuracy: 0.8694 - val_loss: 0.3464 - val_accuracy: 0.8678\n",
            "Epoch 33/100\n",
            "129/129 [==============================] - 42s 327ms/step - loss: 0.3417 - accuracy: 0.8696 - val_loss: 0.3463 - val_accuracy: 0.8678\n",
            "Epoch 34/100\n",
            "129/129 [==============================] - 42s 324ms/step - loss: 0.3418 - accuracy: 0.8692 - val_loss: 0.3458 - val_accuracy: 0.8678\n",
            "Epoch 35/100\n",
            "129/129 [==============================] - 42s 328ms/step - loss: 0.3415 - accuracy: 0.8694 - val_loss: 0.3461 - val_accuracy: 0.8667\n",
            "Epoch 36/100\n",
            "129/129 [==============================] - 42s 323ms/step - loss: 0.3416 - accuracy: 0.8699 - val_loss: 0.3449 - val_accuracy: 0.8678\n",
            "Epoch 37/100\n",
            "129/129 [==============================] - 41s 321ms/step - loss: 0.3415 - accuracy: 0.8694 - val_loss: 0.3470 - val_accuracy: 0.8678\n",
            "Epoch 38/100\n",
            "129/129 [==============================] - 41s 321ms/step - loss: 0.3412 - accuracy: 0.8695 - val_loss: 0.3469 - val_accuracy: 0.8678\n",
            "Epoch 39/100\n",
            "129/129 [==============================] - 42s 323ms/step - loss: 0.3409 - accuracy: 0.8695 - val_loss: 0.3450 - val_accuracy: 0.8678\n",
            "Epoch 40/100\n",
            "129/129 [==============================] - 40s 313ms/step - loss: 0.3411 - accuracy: 0.8690 - val_loss: 0.3447 - val_accuracy: 0.8678\n",
            "Epoch 41/100\n",
            "129/129 [==============================] - 39s 304ms/step - loss: 0.3405 - accuracy: 0.8695 - val_loss: 0.3434 - val_accuracy: 0.8667\n",
            "Epoch 42/100\n",
            "129/129 [==============================] - 40s 311ms/step - loss: 0.3406 - accuracy: 0.8695 - val_loss: 0.3468 - val_accuracy: 0.8699\n",
            "Epoch 43/100\n",
            "129/129 [==============================] - 40s 312ms/step - loss: 0.3406 - accuracy: 0.8691 - val_loss: 0.3471 - val_accuracy: 0.8699\n",
            "Epoch 44/100\n",
            "129/129 [==============================] - 41s 315ms/step - loss: 0.3405 - accuracy: 0.8692 - val_loss: 0.3452 - val_accuracy: 0.8678\n",
            "Epoch 45/100\n",
            "129/129 [==============================] - 41s 315ms/step - loss: 0.3403 - accuracy: 0.8691 - val_loss: 0.3450 - val_accuracy: 0.8667\n",
            "Epoch 46/100\n",
            "129/129 [==============================] - 41s 317ms/step - loss: 0.3403 - accuracy: 0.8695 - val_loss: 0.3449 - val_accuracy: 0.8667\n",
            "Epoch 47/100\n",
            "129/129 [==============================] - 40s 313ms/step - loss: 0.3402 - accuracy: 0.8692 - val_loss: 0.3461 - val_accuracy: 0.8699\n",
            "Epoch 48/100\n",
            "129/129 [==============================] - 38s 293ms/step - loss: 0.3403 - accuracy: 0.8697 - val_loss: 0.3445 - val_accuracy: 0.8667\n",
            "Epoch 49/100\n",
            "129/129 [==============================] - 39s 301ms/step - loss: 0.3400 - accuracy: 0.8694 - val_loss: 0.3448 - val_accuracy: 0.8678\n",
            "Epoch 50/100\n",
            "129/129 [==============================] - 39s 302ms/step - loss: 0.3400 - accuracy: 0.8692 - val_loss: 0.3451 - val_accuracy: 0.8667\n",
            "Epoch 51/100\n",
            "129/129 [==============================] - 38s 298ms/step - loss: 0.3396 - accuracy: 0.8695 - val_loss: 0.3465 - val_accuracy: 0.8699\n",
            "Epoch 52/100\n",
            "129/129 [==============================] - 38s 296ms/step - loss: 0.3398 - accuracy: 0.8702 - val_loss: 0.3464 - val_accuracy: 0.8678\n",
            "Epoch 53/100\n",
            "129/129 [==============================] - 39s 306ms/step - loss: 0.3398 - accuracy: 0.8694 - val_loss: 0.3461 - val_accuracy: 0.8678\n",
            "Epoch 54/100\n",
            "129/129 [==============================] - 40s 307ms/step - loss: 0.3395 - accuracy: 0.8692 - val_loss: 0.3465 - val_accuracy: 0.8678\n",
            "Epoch 55/100\n",
            "129/129 [==============================] - 41s 316ms/step - loss: 0.3396 - accuracy: 0.8692 - val_loss: 0.3440 - val_accuracy: 0.8667\n",
            "Epoch 56/100\n",
            "129/129 [==============================] - 39s 305ms/step - loss: 0.3395 - accuracy: 0.8697 - val_loss: 0.3449 - val_accuracy: 0.8678\n",
            "Epoch 57/100\n",
            "129/129 [==============================] - 38s 291ms/step - loss: 0.3394 - accuracy: 0.8692 - val_loss: 0.3484 - val_accuracy: 0.8699\n",
            "Epoch 58/100\n",
            "129/129 [==============================] - 40s 308ms/step - loss: 0.3393 - accuracy: 0.8694 - val_loss: 0.3480 - val_accuracy: 0.8667\n",
            "Epoch 59/100\n",
            "129/129 [==============================] - 40s 307ms/step - loss: 0.3389 - accuracy: 0.8691 - val_loss: 0.3469 - val_accuracy: 0.8678\n",
            "Epoch 60/100\n",
            "129/129 [==============================] - 39s 306ms/step - loss: 0.3390 - accuracy: 0.8694 - val_loss: 0.3447 - val_accuracy: 0.8678\n",
            "Epoch 61/100\n",
            "129/129 [==============================] - 37s 289ms/step - loss: 0.3388 - accuracy: 0.8697 - val_loss: 0.3454 - val_accuracy: 0.8667\n",
            "Epoch 62/100\n",
            "129/129 [==============================] - 40s 306ms/step - loss: 0.3389 - accuracy: 0.8696 - val_loss: 0.3475 - val_accuracy: 0.8678\n",
            "Epoch 63/100\n",
            "129/129 [==============================] - 40s 311ms/step - loss: 0.3388 - accuracy: 0.8696 - val_loss: 0.3474 - val_accuracy: 0.8678\n",
            "Epoch 64/100\n",
            "129/129 [==============================] - 49s 380ms/step - loss: 0.3388 - accuracy: 0.8696 - val_loss: 0.3476 - val_accuracy: 0.8678\n",
            "Epoch 65/100\n",
            "129/129 [==============================] - 52s 402ms/step - loss: 0.3387 - accuracy: 0.8696 - val_loss: 0.3457 - val_accuracy: 0.8667\n",
            "Epoch 66/100\n",
            "129/129 [==============================] - 49s 380ms/step - loss: 0.3386 - accuracy: 0.8691 - val_loss: 0.3467 - val_accuracy: 0.8667\n",
            "Epoch 67/100\n",
            "129/129 [==============================] - 46s 355ms/step - loss: 0.3382 - accuracy: 0.8696 - val_loss: 0.3440 - val_accuracy: 0.8667\n",
            "Epoch 68/100\n",
            "129/129 [==============================] - 43s 331ms/step - loss: 0.3384 - accuracy: 0.8694 - val_loss: 0.3438 - val_accuracy: 0.8667\n",
            "Epoch 69/100\n",
            "129/129 [==============================] - 43s 331ms/step - loss: 0.3382 - accuracy: 0.8701 - val_loss: 0.3456 - val_accuracy: 0.8699\n",
            "Epoch 70/100\n",
            "129/129 [==============================] - 43s 334ms/step - loss: 0.3380 - accuracy: 0.8696 - val_loss: 0.3443 - val_accuracy: 0.8667\n",
            "Epoch 71/100\n",
            "129/129 [==============================] - 44s 342ms/step - loss: 0.3383 - accuracy: 0.8699 - val_loss: 0.3463 - val_accuracy: 0.8678\n",
            "Epoch 72/100\n",
            "129/129 [==============================] - 43s 333ms/step - loss: 0.3379 - accuracy: 0.8697 - val_loss: 0.3440 - val_accuracy: 0.8667\n",
            "Epoch 73/100\n",
            "129/129 [==============================] - 42s 323ms/step - loss: 0.3380 - accuracy: 0.8699 - val_loss: 0.3484 - val_accuracy: 0.8678\n",
            "Epoch 74/100\n",
            "129/129 [==============================] - 41s 318ms/step - loss: 0.3377 - accuracy: 0.8695 - val_loss: 0.3495 - val_accuracy: 0.8689\n",
            "Epoch 75/100\n",
            "129/129 [==============================] - 41s 316ms/step - loss: 0.3377 - accuracy: 0.8699 - val_loss: 0.3477 - val_accuracy: 0.8689\n",
            "Epoch 76/100\n",
            "129/129 [==============================] - 40s 306ms/step - loss: 0.3379 - accuracy: 0.8696 - val_loss: 0.3441 - val_accuracy: 0.8667\n",
            "Epoch 77/100\n",
            "129/129 [==============================] - 41s 315ms/step - loss: 0.3377 - accuracy: 0.8697 - val_loss: 0.3448 - val_accuracy: 0.8667\n",
            "Epoch 78/100\n",
            "129/129 [==============================] - 40s 314ms/step - loss: 0.3376 - accuracy: 0.8695 - val_loss: 0.3454 - val_accuracy: 0.8667\n",
            "Epoch 79/100\n",
            "129/129 [==============================] - 40s 311ms/step - loss: 0.3376 - accuracy: 0.8700 - val_loss: 0.3462 - val_accuracy: 0.8699\n",
            "Epoch 80/100\n",
            "129/129 [==============================] - 41s 315ms/step - loss: 0.3378 - accuracy: 0.8697 - val_loss: 0.3439 - val_accuracy: 0.8667\n",
            "Epoch 81/100\n",
            "129/129 [==============================] - 40s 312ms/step - loss: 0.3377 - accuracy: 0.8695 - val_loss: 0.3466 - val_accuracy: 0.8678\n",
            "Epoch 82/100\n",
            "129/129 [==============================] - 40s 314ms/step - loss: 0.3376 - accuracy: 0.8697 - val_loss: 0.3481 - val_accuracy: 0.8678\n",
            "Epoch 83/100\n",
            "129/129 [==============================] - 40s 311ms/step - loss: 0.3379 - accuracy: 0.8695 - val_loss: 0.3461 - val_accuracy: 0.8689\n",
            "Epoch 84/100\n",
            "129/129 [==============================] - 38s 293ms/step - loss: 0.3375 - accuracy: 0.8700 - val_loss: 0.3459 - val_accuracy: 0.8678\n",
            "Epoch 85/100\n",
            "129/129 [==============================] - 39s 306ms/step - loss: 0.3371 - accuracy: 0.8699 - val_loss: 0.3442 - val_accuracy: 0.8667\n",
            "Epoch 86/100\n",
            "129/129 [==============================] - 39s 306ms/step - loss: 0.3373 - accuracy: 0.8694 - val_loss: 0.3449 - val_accuracy: 0.8667\n",
            "Epoch 87/100\n",
            "129/129 [==============================] - 40s 308ms/step - loss: 0.3370 - accuracy: 0.8697 - val_loss: 0.3464 - val_accuracy: 0.8667\n",
            "Epoch 88/100\n",
            "129/129 [==============================] - 38s 294ms/step - loss: 0.3373 - accuracy: 0.8692 - val_loss: 0.3441 - val_accuracy: 0.8667\n",
            "Epoch 89/100\n",
            "129/129 [==============================] - 39s 307ms/step - loss: 0.3374 - accuracy: 0.8700 - val_loss: 0.3452 - val_accuracy: 0.8667\n",
            "Epoch 90/100\n",
            "129/129 [==============================] - 39s 306ms/step - loss: 0.3371 - accuracy: 0.8701 - val_loss: 0.3458 - val_accuracy: 0.8678\n",
            "Epoch 91/100\n",
            "129/129 [==============================] - 40s 308ms/step - loss: 0.3372 - accuracy: 0.8699 - val_loss: 0.3463 - val_accuracy: 0.8667\n",
            "Epoch 92/100\n",
            "129/129 [==============================] - 43s 335ms/step - loss: 0.3373 - accuracy: 0.8696 - val_loss: 0.3447 - val_accuracy: 0.8667\n",
            "Epoch 93/100\n",
            "129/129 [==============================] - 43s 338ms/step - loss: 0.3372 - accuracy: 0.8699 - val_loss: 0.3451 - val_accuracy: 0.8667\n",
            "Epoch 94/100\n",
            "129/129 [==============================] - 44s 344ms/step - loss: 0.3370 - accuracy: 0.8696 - val_loss: 0.3452 - val_accuracy: 0.8667\n",
            "Epoch 95/100\n",
            "129/129 [==============================] - 47s 365ms/step - loss: 0.3367 - accuracy: 0.8701 - val_loss: 0.3477 - val_accuracy: 0.8689\n",
            "Epoch 96/100\n",
            "129/129 [==============================] - 41s 320ms/step - loss: 0.3367 - accuracy: 0.8690 - val_loss: 0.3433 - val_accuracy: 0.8667\n",
            "Epoch 97/100\n",
            "129/129 [==============================] - 42s 324ms/step - loss: 0.3367 - accuracy: 0.8699 - val_loss: 0.3436 - val_accuracy: 0.8678\n",
            "Epoch 98/100\n",
            "129/129 [==============================] - 41s 320ms/step - loss: 0.3371 - accuracy: 0.8694 - val_loss: 0.3451 - val_accuracy: 0.8667\n",
            "Epoch 99/100\n",
            "129/129 [==============================] - 42s 326ms/step - loss: 0.3369 - accuracy: 0.8696 - val_loss: 0.3476 - val_accuracy: 0.8667\n",
            "Epoch 100/100\n",
            "129/129 [==============================] - 43s 337ms/step - loss: 0.3367 - accuracy: 0.8701 - val_loss: 0.3462 - val_accuracy: 0.8667\n",
            "72/72 [==============================] - 4s 57ms/step - loss: 0.3211 - accuracy: 0.8801\n",
            "Test Accuracy: 0.8801400065422058\n",
            "72/72 [==============================] - 6s 84ms/step\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.92      0.84      0.88      1157\n",
            "           1       0.85      0.92      0.88      1129\n",
            "\n",
            "    accuracy                           0.88      2286\n",
            "   macro avg       0.88      0.88      0.88      2286\n",
            "weighted avg       0.88      0.88      0.88      2286\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-24-1af1b6031aba>:98: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  save_model(model, 'lstm.h5')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Checking the LSTM model"
      ],
      "metadata": {
        "id": "xsncDtdo-Qua"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import joblib\n",
        "from tensorflow.keras.models import load_model\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "import numpy as np\n",
        "import re\n",
        "import pandas as pd\n",
        "\n",
        "# Load the label encoder, tokenizer, max_text_length, and scaler\n",
        "label_encoder = joblib.load('label_encoder.pkl')\n",
        "tokenizer = joblib.load('tokenizer.pkl')\n",
        "max_text_length = joblib.load('max_text_length.pkl')\n",
        "scaler = joblib.load('scaler.pkl')\n",
        "\n",
        "# Load the trained model\n",
        "model = load_model('lstm.h5')\n",
        "\n",
        "# Load dataset from Kaggle path\n",
        "dataset_path = \"dataset_phishing.csv\"\n",
        "dataset = pd.read_csv(dataset_path)\n",
        "\n",
        "# Function to check URL\n",
        "def check_url(url):\n",
        "    # Check if the URL is in the dataset\n",
        "    if url in dataset['url'].values:\n",
        "        # Get features from the dataset\n",
        "        data_row = dataset[dataset['url'] == url].iloc[0]\n",
        "        ip_present = data_row['ip']\n",
        "        dns_record = data_row['dns_record']\n",
        "        google_index = data_row['google_index']\n",
        "    else:\n",
        "        # Compute features based on the URL\n",
        "        ip_present = 1 if re.search(r'\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}', url) is not None else 0\n",
        "        dns_record = 1 if 'dns' in url else 0\n",
        "        google_index = 1 if 'google' in url else 0\n",
        "\n",
        "    # Tokenize and pad the test URL\n",
        "    test_sequence = tokenizer.texts_to_sequences([url])\n",
        "    test_sequence_padded = pad_sequences(test_sequence, maxlen=max_text_length, padding='post')\n",
        "\n",
        "    # Compute URL length\n",
        "    length_url = len(url)\n",
        "    print(f'URL Length: {length_url}')  # Print URL length\n",
        "\n",
        "    # Compute hostname length\n",
        "    length_hostname = len(url.split('//')[-1].split('/')[0])\n",
        "    print(f'Hostname Length: {length_hostname}')  # Print hostname length\n",
        "\n",
        "    # Print IP address presence\n",
        "    print(f'IP Address Presence: {ip_present}')\n",
        "\n",
        "    # Print DNS record presence\n",
        "    print(f'DNS Record Presence: {dns_record}')\n",
        "\n",
        "    # Print Google index presence\n",
        "    print(f'Google Index Presence: {google_index}')\n",
        "\n",
        "    # Scale the URL length, hostname length, IP address presence, DNS record presence, and Google index presence\n",
        "    features = [[length_url, length_hostname, ip_present, dns_record, google_index]]\n",
        "    scaled_features = scaler.transform(features)\n",
        "\n",
        "    # Make prediction\n",
        "    prediction = model.predict([test_sequence_padded, scaled_features])\n",
        "\n",
        "    # Print prediction\n",
        "    if prediction > 0.5:\n",
        "        return \"The URL is predicted to be phishing.\"\n",
        "    else:\n",
        "        return \"The URL is predicted to be legitimate.\"\n",
        "\n",
        "# Example usage\n",
        "test_url = \"http://pentaho-bi-suite.blogspot.com\"\n",
        "result = check_url(test_url)\n",
        "print(result)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "291h-Feyy89d",
        "outputId": "739f029a-6639-4302-ddf6-c67c943ac935"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "URL Length: 36\n",
            "Hostname Length: 29\n",
            "IP Address Presence: 0\n",
            "DNS Record Presence: 0\n",
            "Google Index Presence: 0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:6 out of the last 77 calls to <function Model.make_predict_function.<locals>.predict_function at 0x78ba71eebbe0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 1s 1s/step\n",
            "The URL is predicted to be legitimate.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# GRU"
      ],
      "metadata": {
        "id": "Q03NJsTU-Wsp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Embedding, GRU, Dense, Concatenate\n",
        "from sklearn.metrics import classification_report\n",
        "import numpy as np\n",
        "import joblib\n",
        "from tensorflow.keras.models import save_model\n",
        "\n",
        "# Load dataset from Kaggle path\n",
        "dataset_path = \"dataset_phishing.csv\"\n",
        "dataset = pd.read_csv(dataset_path)\n",
        "\n",
        "# Add URL length as a feature\n",
        "dataset['length_url'] = dataset['url'].apply(len)\n",
        "# Add hostname length as a feature\n",
        "dataset['length_hostname'] = dataset['url'].apply(lambda x: len(x.split('//')[-1].split('/')[0]))\n",
        "\n",
        "# Extract numerical features including IP address presence, DNS record presence, and Google index presence\n",
        "dataset['ip'] = dataset['ip']\n",
        "dataset['dns_record'] = dataset['dns_record']\n",
        "dataset['google_index'] = dataset['google_index']\n",
        "\n",
        "# Encode categorical labels\n",
        "label_encoder = LabelEncoder()\n",
        "dataset['status'] = label_encoder.fit_transform(dataset['status'])\n",
        "\n",
        "# Save the label encoder\n",
        "joblib.dump(label_encoder, 'label_encoder.pkl')\n",
        "\n",
        "# Text preprocessing\n",
        "texts = dataset['url'].values  # Assuming 'url' is the text column\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(texts)\n",
        "sequences = tokenizer.texts_to_sequences(texts)\n",
        "max_text_length = max([len(seq) for seq in sequences])\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "\n",
        "# Save the tokenizer\n",
        "joblib.dump(tokenizer, 'tokenizer.pkl')\n",
        "joblib.dump(max_text_length, 'max_text_length.pkl')\n",
        "\n",
        "# Padding sequences\n",
        "X_text = pad_sequences(sequences, maxlen=max_text_length, padding='post')\n",
        "\n",
        "# Extract numerical features\n",
        "X_num = dataset[['length_url', 'length_hostname', 'ip', 'dns_record', 'google_index']].values\n",
        "scaler = StandardScaler()\n",
        "X_num = scaler.fit_transform(X_num)\n",
        "\n",
        "# Save the scaler\n",
        "joblib.dump(scaler, 'scaler.pkl')\n",
        "\n",
        "# Split features and labels\n",
        "y = dataset['status']\n",
        "X_train_text, X_test_text, X_train_num, X_test_num, y_train, y_test = train_test_split(\n",
        "    X_text, X_num, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Define GRU model for textual features\n",
        "input_text = Input(shape=(max_text_length,))\n",
        "embedding_layer = Embedding(vocab_size, 100)(input_text)\n",
        "gru_layer = GRU(128)(embedding_layer)\n",
        "\n",
        "# Define dense layer for numerical features\n",
        "input_num = Input(shape=(X_train_num.shape[1],))\n",
        "dense_layer = Dense(64, activation='relu')(input_num)\n",
        "\n",
        "# Concatenate text and numerical features\n",
        "concatenated = Concatenate()([gru_layer, dense_layer])\n",
        "output = Dense(1, activation='sigmoid')(concatenated)\n",
        "\n",
        "# Define combined model\n",
        "model = Model(inputs=[input_text, input_num], outputs=output)\n",
        "\n",
        "# Compile model\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train model\n",
        "model.fit([X_train_text, X_train_num], y_train, epochs=100, batch_size=64, validation_split=0.1)\n",
        "\n",
        "# Evaluate model\n",
        "loss, accuracy = model.evaluate([X_test_text, X_test_num], y_test)\n",
        "print(f'Test Accuracy: {accuracy}')\n",
        "\n",
        "# Generate predictions for test set\n",
        "y_pred_probs = model.predict([X_test_text, X_test_num])\n",
        "y_pred = (y_pred_probs > 0.5).astype(int)\n",
        "\n",
        "# Print classification report\n",
        "print(classification_report(y_test, y_pred))\n",
        "\n",
        "# Save the trained model\n",
        "save_model(model, 'gru.h5')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "hPyqI9lw1Sb1",
        "outputId": "955f182e-4f89-4ea8-d802-f601ceaea0be"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "129/129 [==============================] - 40s 278ms/step - loss: 0.4947 - accuracy: 0.8436 - val_loss: 0.3849 - val_accuracy: 0.8579\n",
            "Epoch 2/100\n",
            "129/129 [==============================] - 36s 278ms/step - loss: 0.3705 - accuracy: 0.8641 - val_loss: 0.3612 - val_accuracy: 0.8656\n",
            "Epoch 3/100\n",
            "129/129 [==============================] - 33s 258ms/step - loss: 0.3624 - accuracy: 0.8666 - val_loss: 0.3578 - val_accuracy: 0.8678\n",
            "Epoch 4/100\n",
            "129/129 [==============================] - 36s 281ms/step - loss: 0.3595 - accuracy: 0.8678 - val_loss: 0.3577 - val_accuracy: 0.8678\n",
            "Epoch 5/100\n",
            "129/129 [==============================] - 34s 267ms/step - loss: 0.3574 - accuracy: 0.8681 - val_loss: 0.3553 - val_accuracy: 0.8678\n",
            "Epoch 6/100\n",
            "129/129 [==============================] - 41s 316ms/step - loss: 0.3555 - accuracy: 0.8684 - val_loss: 0.3547 - val_accuracy: 0.8667\n",
            "Epoch 7/100\n",
            "129/129 [==============================] - 34s 261ms/step - loss: 0.3537 - accuracy: 0.8683 - val_loss: 0.3540 - val_accuracy: 0.8667\n",
            "Epoch 8/100\n",
            "129/129 [==============================] - 35s 273ms/step - loss: 0.3524 - accuracy: 0.8695 - val_loss: 0.3552 - val_accuracy: 0.8678\n",
            "Epoch 9/100\n",
            "129/129 [==============================] - 42s 327ms/step - loss: 0.3510 - accuracy: 0.8684 - val_loss: 0.3535 - val_accuracy: 0.8667\n",
            "Epoch 10/100\n",
            "129/129 [==============================] - 34s 266ms/step - loss: 0.3500 - accuracy: 0.8686 - val_loss: 0.3530 - val_accuracy: 0.8667\n",
            "Epoch 11/100\n",
            "129/129 [==============================] - 36s 280ms/step - loss: 0.3494 - accuracy: 0.8686 - val_loss: 0.3514 - val_accuracy: 0.8667\n",
            "Epoch 12/100\n",
            "129/129 [==============================] - 34s 263ms/step - loss: 0.3485 - accuracy: 0.8694 - val_loss: 0.3543 - val_accuracy: 0.8678\n",
            "Epoch 13/100\n",
            "129/129 [==============================] - 36s 277ms/step - loss: 0.3478 - accuracy: 0.8690 - val_loss: 0.3480 - val_accuracy: 0.8667\n",
            "Epoch 14/100\n",
            "129/129 [==============================] - 33s 258ms/step - loss: 0.3474 - accuracy: 0.8690 - val_loss: 0.3474 - val_accuracy: 0.8667\n",
            "Epoch 15/100\n",
            "129/129 [==============================] - 34s 260ms/step - loss: 0.3467 - accuracy: 0.8690 - val_loss: 0.3499 - val_accuracy: 0.8678\n",
            "Epoch 16/100\n",
            "129/129 [==============================] - 35s 269ms/step - loss: 0.3462 - accuracy: 0.8690 - val_loss: 0.3477 - val_accuracy: 0.8667\n",
            "Epoch 17/100\n",
            "129/129 [==============================] - 33s 257ms/step - loss: 0.3460 - accuracy: 0.8692 - val_loss: 0.3494 - val_accuracy: 0.8667\n",
            "Epoch 18/100\n",
            "129/129 [==============================] - 35s 271ms/step - loss: 0.3458 - accuracy: 0.8692 - val_loss: 0.3481 - val_accuracy: 0.8667\n",
            "Epoch 19/100\n",
            "129/129 [==============================] - 33s 255ms/step - loss: 0.3455 - accuracy: 0.8689 - val_loss: 0.3496 - val_accuracy: 0.8667\n",
            "Epoch 20/100\n",
            "129/129 [==============================] - 35s 270ms/step - loss: 0.3450 - accuracy: 0.8691 - val_loss: 0.3472 - val_accuracy: 0.8667\n",
            "Epoch 21/100\n",
            "129/129 [==============================] - 33s 253ms/step - loss: 0.3442 - accuracy: 0.8686 - val_loss: 0.3495 - val_accuracy: 0.8667\n",
            "Epoch 22/100\n",
            "129/129 [==============================] - 34s 262ms/step - loss: 0.3447 - accuracy: 0.8689 - val_loss: 0.3459 - val_accuracy: 0.8667\n",
            "Epoch 23/100\n",
            "129/129 [==============================] - 35s 268ms/step - loss: 0.3442 - accuracy: 0.8694 - val_loss: 0.3478 - val_accuracy: 0.8667\n",
            "Epoch 24/100\n",
            "129/129 [==============================] - 33s 258ms/step - loss: 0.3437 - accuracy: 0.8690 - val_loss: 0.3487 - val_accuracy: 0.8667\n",
            "Epoch 25/100\n",
            "129/129 [==============================] - 35s 274ms/step - loss: 0.3437 - accuracy: 0.8692 - val_loss: 0.3496 - val_accuracy: 0.8667\n",
            "Epoch 26/100\n",
            "129/129 [==============================] - 33s 255ms/step - loss: 0.3433 - accuracy: 0.8692 - val_loss: 0.3479 - val_accuracy: 0.8667\n",
            "Epoch 27/100\n",
            "129/129 [==============================] - 33s 254ms/step - loss: 0.3430 - accuracy: 0.8691 - val_loss: 0.3487 - val_accuracy: 0.8667\n",
            "Epoch 28/100\n",
            "129/129 [==============================] - 35s 270ms/step - loss: 0.3431 - accuracy: 0.8688 - val_loss: 0.3466 - val_accuracy: 0.8667\n",
            "Epoch 29/100\n",
            "129/129 [==============================] - 33s 259ms/step - loss: 0.3425 - accuracy: 0.8694 - val_loss: 0.3454 - val_accuracy: 0.8667\n",
            "Epoch 30/100\n",
            "129/129 [==============================] - 39s 302ms/step - loss: 0.3422 - accuracy: 0.8694 - val_loss: 0.3447 - val_accuracy: 0.8667\n",
            "Epoch 31/100\n",
            "129/129 [==============================] - 50s 386ms/step - loss: 0.3422 - accuracy: 0.8691 - val_loss: 0.3480 - val_accuracy: 0.8667\n",
            "Epoch 32/100\n",
            "129/129 [==============================] - 43s 337ms/step - loss: 0.3424 - accuracy: 0.8688 - val_loss: 0.3459 - val_accuracy: 0.8667\n",
            "Epoch 33/100\n",
            "129/129 [==============================] - 41s 315ms/step - loss: 0.3417 - accuracy: 0.8695 - val_loss: 0.3451 - val_accuracy: 0.8667\n",
            "Epoch 34/100\n",
            "129/129 [==============================] - 42s 329ms/step - loss: 0.3419 - accuracy: 0.8692 - val_loss: 0.3472 - val_accuracy: 0.8667\n",
            "Epoch 35/100\n",
            "129/129 [==============================] - 41s 321ms/step - loss: 0.3413 - accuracy: 0.8692 - val_loss: 0.3504 - val_accuracy: 0.8699\n",
            "Epoch 36/100\n",
            "129/129 [==============================] - 39s 302ms/step - loss: 0.3413 - accuracy: 0.8695 - val_loss: 0.3469 - val_accuracy: 0.8689\n",
            "Epoch 37/100\n",
            "129/129 [==============================] - 39s 297ms/step - loss: 0.3414 - accuracy: 0.8692 - val_loss: 0.3477 - val_accuracy: 0.8667\n",
            "Epoch 38/100\n",
            "129/129 [==============================] - 39s 305ms/step - loss: 0.3408 - accuracy: 0.8691 - val_loss: 0.3457 - val_accuracy: 0.8667\n",
            "Epoch 39/100\n",
            "129/129 [==============================] - 40s 311ms/step - loss: 0.3405 - accuracy: 0.8697 - val_loss: 0.3493 - val_accuracy: 0.8667\n",
            "Epoch 40/100\n",
            "129/129 [==============================] - 36s 282ms/step - loss: 0.3413 - accuracy: 0.8689 - val_loss: 0.3457 - val_accuracy: 0.8667\n",
            "Epoch 41/100\n",
            "129/129 [==============================] - 37s 288ms/step - loss: 0.3406 - accuracy: 0.8689 - val_loss: 0.3469 - val_accuracy: 0.8667\n",
            "Epoch 42/100\n",
            "129/129 [==============================] - 35s 272ms/step - loss: 0.3403 - accuracy: 0.8694 - val_loss: 0.3459 - val_accuracy: 0.8667\n",
            "Epoch 43/100\n",
            "129/129 [==============================] - 36s 277ms/step - loss: 0.3406 - accuracy: 0.8691 - val_loss: 0.3465 - val_accuracy: 0.8667\n",
            "Epoch 44/100\n",
            "129/129 [==============================] - 35s 269ms/step - loss: 0.3404 - accuracy: 0.8694 - val_loss: 0.3460 - val_accuracy: 0.8667\n",
            "Epoch 45/100\n",
            "129/129 [==============================] - 37s 285ms/step - loss: 0.3403 - accuracy: 0.8692 - val_loss: 0.3479 - val_accuracy: 0.8689\n",
            "Epoch 46/100\n",
            "129/129 [==============================] - 37s 284ms/step - loss: 0.3401 - accuracy: 0.8692 - val_loss: 0.3466 - val_accuracy: 0.8667\n",
            "Epoch 47/100\n",
            "129/129 [==============================] - 35s 270ms/step - loss: 0.3398 - accuracy: 0.8692 - val_loss: 0.3453 - val_accuracy: 0.8667\n",
            "Epoch 48/100\n",
            "129/129 [==============================] - 37s 288ms/step - loss: 0.3399 - accuracy: 0.8696 - val_loss: 0.3481 - val_accuracy: 0.8667\n",
            "Epoch 49/100\n",
            "129/129 [==============================] - 36s 277ms/step - loss: 0.3398 - accuracy: 0.8694 - val_loss: 0.3466 - val_accuracy: 0.8667\n",
            "Epoch 50/100\n",
            "129/129 [==============================] - 36s 280ms/step - loss: 0.3397 - accuracy: 0.8691 - val_loss: 0.3467 - val_accuracy: 0.8667\n",
            "Epoch 51/100\n",
            "129/129 [==============================] - 34s 263ms/step - loss: 0.3397 - accuracy: 0.8690 - val_loss: 0.3464 - val_accuracy: 0.8667\n",
            "Epoch 52/100\n",
            "129/129 [==============================] - 36s 281ms/step - loss: 0.3396 - accuracy: 0.8694 - val_loss: 0.3485 - val_accuracy: 0.8689\n",
            "Epoch 53/100\n",
            "129/129 [==============================] - 34s 260ms/step - loss: 0.3391 - accuracy: 0.8696 - val_loss: 0.3461 - val_accuracy: 0.8667\n",
            "Epoch 54/100\n",
            "129/129 [==============================] - 35s 274ms/step - loss: 0.3392 - accuracy: 0.8696 - val_loss: 0.3474 - val_accuracy: 0.8667\n",
            "Epoch 55/100\n",
            "129/129 [==============================] - 33s 256ms/step - loss: 0.3394 - accuracy: 0.8696 - val_loss: 0.3461 - val_accuracy: 0.8667\n",
            "Epoch 56/100\n",
            "129/129 [==============================] - 34s 266ms/step - loss: 0.3395 - accuracy: 0.8692 - val_loss: 0.3466 - val_accuracy: 0.8667\n",
            "Epoch 57/100\n",
            "129/129 [==============================] - 35s 269ms/step - loss: 0.3391 - accuracy: 0.8699 - val_loss: 0.3471 - val_accuracy: 0.8667\n",
            "Epoch 58/100\n",
            "129/129 [==============================] - 34s 262ms/step - loss: 0.3392 - accuracy: 0.8697 - val_loss: 0.3457 - val_accuracy: 0.8667\n",
            "Epoch 59/100\n",
            "129/129 [==============================] - 36s 275ms/step - loss: 0.3386 - accuracy: 0.8700 - val_loss: 0.3479 - val_accuracy: 0.8689\n",
            "Epoch 60/100\n",
            "129/129 [==============================] - 33s 256ms/step - loss: 0.3392 - accuracy: 0.8699 - val_loss: 0.3457 - val_accuracy: 0.8667\n",
            "Epoch 61/100\n",
            "129/129 [==============================] - 36s 276ms/step - loss: 0.3389 - accuracy: 0.8692 - val_loss: 0.3455 - val_accuracy: 0.8667\n",
            "Epoch 62/100\n",
            "129/129 [==============================] - 34s 260ms/step - loss: 0.3387 - accuracy: 0.8694 - val_loss: 0.3458 - val_accuracy: 0.8667\n",
            "Epoch 63/100\n",
            "129/129 [==============================] - 34s 267ms/step - loss: 0.3383 - accuracy: 0.8692 - val_loss: 0.3469 - val_accuracy: 0.8667\n",
            "Epoch 64/100\n",
            "129/129 [==============================] - 33s 259ms/step - loss: 0.3386 - accuracy: 0.8699 - val_loss: 0.3477 - val_accuracy: 0.8667\n",
            "Epoch 65/100\n",
            "129/129 [==============================] - 33s 257ms/step - loss: 0.3388 - accuracy: 0.8697 - val_loss: 0.3460 - val_accuracy: 0.8667\n",
            "Epoch 66/100\n",
            "129/129 [==============================] - 36s 278ms/step - loss: 0.3384 - accuracy: 0.8696 - val_loss: 0.3456 - val_accuracy: 0.8667\n",
            "Epoch 67/100\n",
            "129/129 [==============================] - 34s 264ms/step - loss: 0.3384 - accuracy: 0.8695 - val_loss: 0.3465 - val_accuracy: 0.8667\n",
            "Epoch 68/100\n",
            "129/129 [==============================] - 36s 276ms/step - loss: 0.3383 - accuracy: 0.8695 - val_loss: 0.3485 - val_accuracy: 0.8689\n",
            "Epoch 69/100\n",
            "129/129 [==============================] - 34s 264ms/step - loss: 0.3385 - accuracy: 0.8694 - val_loss: 0.3460 - val_accuracy: 0.8667\n",
            "Epoch 70/100\n",
            "129/129 [==============================] - 35s 270ms/step - loss: 0.3382 - accuracy: 0.8696 - val_loss: 0.3471 - val_accuracy: 0.8667\n",
            "Epoch 71/100\n",
            "129/129 [==============================] - 34s 262ms/step - loss: 0.3380 - accuracy: 0.8701 - val_loss: 0.3453 - val_accuracy: 0.8667\n",
            "Epoch 72/100\n",
            "129/129 [==============================] - 34s 261ms/step - loss: 0.3381 - accuracy: 0.8695 - val_loss: 0.3478 - val_accuracy: 0.8667\n",
            "Epoch 73/100\n",
            "129/129 [==============================] - 35s 268ms/step - loss: 0.3377 - accuracy: 0.8697 - val_loss: 0.3464 - val_accuracy: 0.8667\n",
            "Epoch 74/100\n",
            "129/129 [==============================] - 34s 262ms/step - loss: 0.3379 - accuracy: 0.8700 - val_loss: 0.3465 - val_accuracy: 0.8667\n",
            "Epoch 75/100\n",
            "129/129 [==============================] - 35s 275ms/step - loss: 0.3380 - accuracy: 0.8699 - val_loss: 0.3453 - val_accuracy: 0.8667\n",
            "Epoch 76/100\n",
            "129/129 [==============================] - 33s 260ms/step - loss: 0.3380 - accuracy: 0.8696 - val_loss: 0.3457 - val_accuracy: 0.8667\n",
            "Epoch 77/100\n",
            "129/129 [==============================] - 36s 277ms/step - loss: 0.3377 - accuracy: 0.8695 - val_loss: 0.3469 - val_accuracy: 0.8667\n",
            "Epoch 78/100\n",
            "129/129 [==============================] - 34s 261ms/step - loss: 0.3376 - accuracy: 0.8696 - val_loss: 0.3454 - val_accuracy: 0.8667\n",
            "Epoch 79/100\n",
            "129/129 [==============================] - 36s 276ms/step - loss: 0.3376 - accuracy: 0.8699 - val_loss: 0.3465 - val_accuracy: 0.8689\n",
            "Epoch 80/100\n",
            "129/129 [==============================] - 34s 260ms/step - loss: 0.3374 - accuracy: 0.8699 - val_loss: 0.3464 - val_accuracy: 0.8667\n",
            "Epoch 81/100\n",
            "129/129 [==============================] - 34s 261ms/step - loss: 0.3377 - accuracy: 0.8691 - val_loss: 0.3454 - val_accuracy: 0.8667\n",
            "Epoch 82/100\n",
            "129/129 [==============================] - 35s 271ms/step - loss: 0.3374 - accuracy: 0.8695 - val_loss: 0.3457 - val_accuracy: 0.8667\n",
            "Epoch 83/100\n",
            "129/129 [==============================] - 34s 264ms/step - loss: 0.3376 - accuracy: 0.8692 - val_loss: 0.3452 - val_accuracy: 0.8667\n",
            "Epoch 84/100\n",
            "129/129 [==============================] - 35s 272ms/step - loss: 0.3372 - accuracy: 0.8699 - val_loss: 0.3491 - val_accuracy: 0.8667\n",
            "Epoch 85/100\n",
            "129/129 [==============================] - 33s 253ms/step - loss: 0.3375 - accuracy: 0.8699 - val_loss: 0.3453 - val_accuracy: 0.8667\n",
            "Epoch 86/100\n",
            "129/129 [==============================] - 34s 267ms/step - loss: 0.3372 - accuracy: 0.8696 - val_loss: 0.3454 - val_accuracy: 0.8689\n",
            "Epoch 87/100\n",
            "129/129 [==============================] - 33s 257ms/step - loss: 0.3374 - accuracy: 0.8700 - val_loss: 0.3453 - val_accuracy: 0.8667\n",
            "Epoch 88/100\n",
            "129/129 [==============================] - 33s 258ms/step - loss: 0.3372 - accuracy: 0.8699 - val_loss: 0.3446 - val_accuracy: 0.8667\n",
            "Epoch 89/100\n",
            "129/129 [==============================] - 49s 383ms/step - loss: 0.3370 - accuracy: 0.8702 - val_loss: 0.3453 - val_accuracy: 0.8667\n",
            "Epoch 90/100\n",
            "129/129 [==============================] - 48s 371ms/step - loss: 0.3371 - accuracy: 0.8699 - val_loss: 0.3467 - val_accuracy: 0.8667\n",
            "Epoch 91/100\n",
            "129/129 [==============================] - 40s 312ms/step - loss: 0.3372 - accuracy: 0.8700 - val_loss: 0.3470 - val_accuracy: 0.8667\n",
            "Epoch 92/100\n",
            "129/129 [==============================] - 41s 316ms/step - loss: 0.3370 - accuracy: 0.8705 - val_loss: 0.3465 - val_accuracy: 0.8667\n",
            "Epoch 93/100\n",
            "129/129 [==============================] - 39s 301ms/step - loss: 0.3372 - accuracy: 0.8701 - val_loss: 0.3462 - val_accuracy: 0.8689\n",
            "Epoch 94/100\n",
            "129/129 [==============================] - 38s 297ms/step - loss: 0.3369 - accuracy: 0.8696 - val_loss: 0.3450 - val_accuracy: 0.8667\n",
            "Epoch 95/100\n",
            "129/129 [==============================] - 40s 307ms/step - loss: 0.3368 - accuracy: 0.8696 - val_loss: 0.3480 - val_accuracy: 0.8689\n",
            "Epoch 96/100\n",
            "129/129 [==============================] - 38s 296ms/step - loss: 0.3363 - accuracy: 0.8697 - val_loss: 0.3435 - val_accuracy: 0.8667\n",
            "Epoch 97/100\n",
            "129/129 [==============================] - 37s 287ms/step - loss: 0.3370 - accuracy: 0.8699 - val_loss: 0.3445 - val_accuracy: 0.8667\n",
            "Epoch 98/100\n",
            "129/129 [==============================] - 38s 293ms/step - loss: 0.3367 - accuracy: 0.8697 - val_loss: 0.3443 - val_accuracy: 0.8667\n",
            "Epoch 99/100\n",
            "129/129 [==============================] - 37s 288ms/step - loss: 0.3367 - accuracy: 0.8699 - val_loss: 0.3451 - val_accuracy: 0.8667\n",
            "Epoch 100/100\n",
            "129/129 [==============================] - 36s 279ms/step - loss: 0.3368 - accuracy: 0.8699 - val_loss: 0.3470 - val_accuracy: 0.8667\n",
            "72/72 [==============================] - 2s 34ms/step - loss: 0.3218 - accuracy: 0.8797\n",
            "Test Accuracy: 0.8797025084495544\n",
            "72/72 [==============================] - 3s 32ms/step\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.92      0.84      0.88      1157\n",
            "           1       0.85      0.92      0.88      1129\n",
            "\n",
            "    accuracy                           0.88      2286\n",
            "   macro avg       0.88      0.88      0.88      2286\n",
            "weighted avg       0.88      0.88      0.88      2286\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-25-ca46553f3e68>:98: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  save_model(model, 'gru.h5')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Checking the GRU model"
      ],
      "metadata": {
        "id": "v6V3_70P-bgi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import joblib\n",
        "from tensorflow.keras.models import load_model\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "import numpy as np\n",
        "import re\n",
        "import pandas as pd\n",
        "\n",
        "# Load the label encoder, tokenizer, max_text_length, and scaler\n",
        "label_encoder = joblib.load('label_encoder.pkl')\n",
        "tokenizer = joblib.load('tokenizer.pkl')\n",
        "max_text_length = joblib.load('max_text_length.pkl')\n",
        "scaler = joblib.load('scaler.pkl')\n",
        "\n",
        "# Load the trained model\n",
        "model = load_model('gru.h5')\n",
        "\n",
        "# Load dataset from Kaggle path\n",
        "dataset_path = \"dataset_phishing.csv\"\n",
        "dataset = pd.read_csv(dataset_path)\n",
        "\n",
        "# Function to check URL\n",
        "def check_url(url):\n",
        "    # Check if the URL is in the dataset\n",
        "    if url in dataset['url'].values:\n",
        "        # Get features from the dataset\n",
        "        data_row = dataset[dataset['url'] == url].iloc[0]\n",
        "        ip_present = data_row['ip']\n",
        "        dns_record = data_row['dns_record']\n",
        "        google_index = data_row['google_index']\n",
        "    else:\n",
        "        # Compute features based on the URL\n",
        "        ip_present = 1 if re.search(r'\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}', url) is not None else 0\n",
        "        dns_record = 1 if 'dns' in url else 0\n",
        "        google_index = 1 if 'google' in url else 0\n",
        "\n",
        "    # Tokenize and pad the test URL\n",
        "    test_sequence = tokenizer.texts_to_sequences([url])\n",
        "    test_sequence_padded = pad_sequences(test_sequence, maxlen=max_text_length, padding='post')\n",
        "\n",
        "    # Compute URL length\n",
        "    length_url = len(url)\n",
        "    print(f'URL Length: {length_url}')  # Print URL length\n",
        "\n",
        "    # Compute hostname length\n",
        "    length_hostname = len(url.split('//')[-1].split('/')[0])\n",
        "    print(f'Hostname Length: {length_hostname}')  # Print hostname length\n",
        "\n",
        "    # Print IP address presence\n",
        "    print(f'IP Address Presence: {ip_present}')\n",
        "\n",
        "    # Print DNS record presence\n",
        "    print(f'DNS Record Presence: {dns_record}')\n",
        "\n",
        "    # Print Google index presence\n",
        "    print(f'Google Index Presence: {google_index}')\n",
        "\n",
        "    # Scale the URL length, hostname length, IP address presence, DNS record presence, and Google index presence\n",
        "    features = [[length_url, length_hostname, ip_present, dns_record, google_index]]\n",
        "    scaled_features = scaler.transform(features)\n",
        "\n",
        "    # Make prediction\n",
        "    prediction = model.predict([test_sequence_padded, scaled_features])\n",
        "\n",
        "    # Print prediction\n",
        "    if prediction > 0.5:\n",
        "        return \"The URL is predicted to be phishing.\"\n",
        "    else:\n",
        "        return \"The URL is predicted to be legitimate.\"\n",
        "\n",
        "# Example usage\n",
        "test_url = \"https://chatgpt.com/?oai-dm=1\"\n",
        "result = check_url(test_url)\n",
        "print(result)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "PVn-Ha9Y425e",
        "outputId": "9ec816fe-75cb-4c50-c411-e497d6d14a3f"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "URL Length: 29\n",
            "Hostname Length: 11\n",
            "IP Address Presence: 0\n",
            "DNS Record Presence: 0\n",
            "Google Index Presence: 0\n",
            "1/1 [==============================] - 1s 730ms/step\n",
            "The URL is predicted to be legitimate.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Graphs"
      ],
      "metadata": {
        "id": "N7YjqxLI-iCf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Accuracy values for different models\n",
        "models = ['CNN', 'RNN', 'RNN with Attention Mechanism', 'R-CNN', 'LSTM', 'GRU']\n",
        "accuracies = [0.9383202195167542, 0.8797025084495544, 0.93438321352005, 0.9387576580047607, 0.8801400065422058, 0.8797025084495544]\n",
        "\n",
        "# Plotting the bar graph\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.bar(models, accuracies, color='skyblue')\n",
        "\n",
        "# Adding title and labels\n",
        "plt.title('Test Accuracy of Different Models')\n",
        "plt.xlabel('Models')\n",
        "plt.ylabel('Test Accuracy')\n",
        "plt.ylim(0.8, 1)  # Setting the y-axis limit for better visualization\n",
        "plt.xticks(rotation=45)  # Rotating the x-axis labels for better readability\n",
        "\n",
        "# Adding accuracy values on top of the bars\n",
        "for i in range(len(models)):\n",
        "    plt.text(i, accuracies[i] + 0.002, f'{accuracies[i]:.3f}', ha='center', va='bottom')\n",
        "\n",
        "# Displaying the plot\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 713
        },
        "id": "RTXKisG45kdN",
        "outputId": "beb396c2-1db1-41bd-d3f1-00ad9a734edb"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x600 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    }
  ]
}